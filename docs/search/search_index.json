{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About","title":"Home"},{"location":"advanced/","text":"Using Fiber with GPUs \u00b6 Fiber supports running workload with GPUs on Kubernetes. First, you need to configure your Kubernetes cluster for GPU support. Check out this guide for details. Currently, we support Nvidia GPU with NVIDIA GPU device plugin used by GCE . Other types of device and device plugin will be supported in the future. Once GPU is set up on your cluster. We can run some tests to see if GPU is ready for use. Let's create a new docker file called fiber-gpu.docker with the following content: FROM nvidia/cuda:10.0-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y python3-pip RUN pip3 install fiber This time, we use nvidia/cuda:10.0-runtime-ubuntu18.04 as our base image. This is because it already have CUDA libraries properly configured. You can also use other base images, but you may need to configure CUDA path by yourself. Check out this guide for details. Using GPU with Fiber master process \u00b6 If the master process needs GPU, then it can be done by providing --gpu argument to fiber command when running your program. Alternatively, you can also specify nvidia.com/gpu limits when you launch fiber master process with kubectl . Checkout here for details. Here, we show how this can be done with fiber command. Let's test if GPU is available by running this test command and it should print out the pod name created: $ fiber run --gpu 1 nvidia-smi ... Created pod: fiber-e3ae700b If everything is configured properly, you should see something similar to this when you get logs for that pod: $ kubectl logs fiber-e3ae700b Wed Feb 5 20:33:04 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.67 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:05.0 Off | 0 | | N/A 35C P8 9W / 70W | 0MiB / 15079MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ When we pass --gpu 1 to fiber command, it created a new pod with 1 GPU allocated to that pod. And we can see in our example, one Nvidia Tesla T4 GPU has been allocated to our pod. Using GPU with Fiber child process \u00b6 If the child process needs GPU, it can be done with Fiber's resource limits . Simply add @fiber.meta(gpu=x) to your child process function that needs GPU, and Fiber will allocate GPU for your function when it runs. Let's create an simple program with runs a child process with PyTorch and GPU. Create a file called fiber_process_gpu.py . import fiber from fiber import SimpleQueue , Process import torch import numpy as np @fiber . meta ( gpu = 1 ) def process_using_gpu ( q ): input_a , input_b = q . get () a = torch . tensor ( input_a ) . cuda () b = torch . tensor ( input_b ) . cuda () c = torch . matmul ( a , b ) . cpu () . numpy () q . put ( c ) def main (): q = SimpleQueue () p = Process ( target = process_using_gpu , args = ( q ,)) p . start () a = np . random . rand ( 3 , 4 ) b = np . random . rand ( 4 , 2 ) q . put (( a , b )) res = q . get () print ( \"Result is\" , res ) p . terminate () p . join () if __name__ == '__main__' : main () In this example, we have a child process which accepts 2 tensors from the master process, calculate their product with GPU and then return the result to the master process. Because this example needs PyTorch and NumPy, we added a few lines to fiber-gpu.docker . The new docker file looks like this: FROM nvidia/cuda:10.0-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y python3-pip RUN pip3 install --no-cache-dir torch == 1 .2.0 torchvision == 0 .4.0 pillow == 6 .1 RUN pip3 install fiber ADD fiber_process_gpu.py /root/fiber_process_gpu.py Run this program with the following command: $ fiber run python3 /root/fiber_process_gpu.py ... Created pod: fiber-384369de Get logs of pod fiber-384369de . $ kubectl logs fiber-384369de Result is ( array ([[ 0 .10797104, 0 .65835916, 0 .95002519, 0 .83939533 ] , [ 0 .06103808, 0 .39594844, 0 .56635164, 0 .61488279 ] , [ 0 .26484163, 0 .75913394, 0 .45325563, 0 .62634138 ]]) , array ([[ 0 .52567844, 0 .47349188 ] , [ 0 .58966787, 0 .05199646 ] , [ 0 .10254589, 0 .37998549 ] , [ 0 .5943244 , 0 .02409804 ]])) We see that the master process has successfully printed out the product of 2 tensors calculated on GPU from its child process. Working with persistent storage \u00b6 It is also very important to get your training logs after your training jobs is done. Because the disk space inside running container is not persistent, we need to store logs and outputs of our program on persistent storage. fiber command line tool provided a way to use Kubernetes' PersistentVolumeClaim to storage program output data. Here, we create an NFS Persistent Volume claim following instructions here . # create persistent volume claim on GCP $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/provisioner/nfs-server-gce-pv.yaml # create nfs server $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-server-rc.yaml # create nfs server service $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-server-service.yaml # create persistent volume $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-pv.yaml # create nfs persistent volume claim $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-pvc.yaml This will create an NFS persistent volume claim called \"nfs\" with size 1M. Once we have it done, we can try out this feature by: fiber run -v nfs bash -c 'echo \"This file is persisted across runs\" > /persistent/foo.txt' What this command does is in addition to create a new job, it mounted the persistent volume claim nfs to path /persistent inside the container. Then we create a file called /persistent/foo.txt with some content. Then we can retrieve the content of this file from a different job: $ fiber run -v nfs cat /persistent/foo.txt ... Created pod: fiber-88e2197a $ kubectl logs fiber-88e2197a This file is persisted across runs Error handling \u00b6 Fiber's Pool supports error handling by default. This means when a Pool is created and all the Pool workers are up, if one of the worker crashes in the middle of the computation, then the tasks that was working on by that worker will be retried. Those tasks will be sent to other healthy workers and the master process will not be interrupted by this crash. At the same time, the crashed worker will be cleaned up and a new worker will be started to replace that crashed Pool worker. A Pool with a list of tasks can be viewed as a contract between the process that created the Pool and all worker processes. We define some concepts as below: Task function is a function that is passed to Pool.apply() or other similar functions like Pool.map() as func argument. Task arguments are Python objects that are passed to task functions when they run. Task is a task function combined with task arguments. Task result is the result returned by a task function. Task queue is a queue used internally by a Process Pool to store all tasks. All tasks are distributed to worker processes through task queue. When a user creates a new Pool, a Pool and associated task queue, result queue and pending table are created. All worker processes are also created. First, the Pool will put all tasks into the task queue, which is shared between the master process and worker processes. Each of the workers will fetch a single task (or a batch of tasks) from the task queue as task arguments, then run task function with task arguments. Each time a task is removed from the task queue, an entry in the pending table is added. The entry will have the worker process\u2019 id as its key and the task as its value. Once the worker finished that task, it will put the result in the result queue. And Pool will remove the entry associated with that task from the pending table. If a worker process fails in the middle of processing (Worker 3 in the above diagram). Then its failure will be detected by the Pool which serves as a process manager of all worker processes and constantly checks with Peloton to get all its workers\u2019 states. Then the Pool will put the pending task from the pending table back to the task queue if the previously failed process has a pending task. Then it will start a new worker process (Worker 5) to replace the previously failed process and connect the newly created worker process to a task queue and result queue. In this way, we can make sure that all the tasks that are put into the task queue get process by one of the worker processes. Note that the automatic retry should be only turned on if the task function is idempotent . Extensions to standard multiprocessing API \u00b6 Ring \u00b6 A Ring in fiber stands for a list of processes who work collectively together. Unlike Pool , Ring doesn't have the concept of master process and worker process. All the members inside the Ring shared basically the same responsibility. Each node in the ring usually only talk to it's left and right neighbors. Each member in the Ring has a rank which range from 0 to the number of nodes - 1. Usually ring node 0 has some specially responsibility to gather additional information and usually also serves as a control node. The ring topology is very common in machine learning when doing distributed SGD . Examples include torch.distributed , Horovod , etc. But generally it's really hard to start this kind of workload on a computer cluster, especially when setting up the communication between different nodes are needed. Fiber provides the Ring class to simplify this process. Fiber handles the starting of all the processes and provides network information to the ring-forming framework. The end user only needs to provide an initialization function and the target function that each node runs, and Fiber will handle the rest for the user. An example of using Ring with torch.distributed can be found in examples/ring.py in Fiber's source code repo. As mentioned above, Ring needs a initialization function and a target function. In this example, pytorch_ring_init is the initialization function. In it, we initialize PyTorch and set environment variables to tell PyTorch where is the master node so that all the nodes can be discovered by PyTorch. And pytorch_run_sgd is the target function that does distributed SGD on each of the Ring node. Fiber will first call pytorch_ring_init on each of the node to set up the ring and then call pytorch_run_sgd to do the actual distributed SGD. Fiber Ring . A Fiber Ring with 4 nodes is depicted. Ring node 0 and ring node 3 run on the same machine but in two different containers. Ring nodes 1 and 2 both run on a separate machine. All these processes collectively run a copy of the same function and communicate with each other during the run. Ring demo \u00b6 To run everything, we can re-use the docker file from the previous section and download examples/ring.py and put it into the same directory as your docker file. This demo shows how to run a Ring on Kubernetes with GPU. Add a new line to the docker file ADD ring.py /root/ring.py and the new docker file looks like this: FROM nvidia/cuda:10.0-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y python3-pip RUN pip3 install --no-cache-dir torch == 1 .2.0 torchvision == 0 .4.0 pillow == 6 .1 RUN pip3 install fiber ADD fiber_process_gpu.py /root/fiber_process_gpu.py ADD ring.py /root/ring.py Now we run everything with this command: $ fiber run --gpu 1 python3 /root/ring.py ... Created pod: fiber-dbb2d4d9 Check the logs our the job: $ kubectl logs fiber-dbb2d4d9 pytorch ring init, rank 0 2 0 eth0 10 .12.0.116 48707 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz 100 .1%Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz 113 .5%Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz 100 .4%Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz 180 .4%Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw Processing... Done! Rank 0 , epoch 0 : 0 .5177083023861527 Rank 0 , epoch 1 : 0 .12978197557569693 ... This means ring node 0 has been started and running! What about the other nodes in the ring? We can first get the pods that other process are running by: $ kubectl get po | grep process process-3-03e3a1b6 1 /1 Running 0 3m12s In this example, we only have one addition node in the ring and the pod name is process-3-03e3a1b6 . Then we can get it's logs by: $ kubectl logs process-3-03e3a1b6 pytorch ring init, rank 1 2 1 eth0 10 .12.0.116 48707 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz 100 .1%Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz 113 .5%Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz 100 .4%Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz 180 .4%Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw Processing... Done! Rank 1 , epoch 0 : 0 .5106956963933734 Rank 1 , epoch 1 : 0 .12546868318084206 ... It showed we have successfully set up our distributed SGD application with Fiber and PyTorch and GPU! AsyncManager \u00b6 Python's multiprocessing provides a Manager type to share data between different processes. A manager manages a Python object and it accepts method calls from remote so that the state of the managed object can be changed. The stateful storage nature of Manager make it suitable to be use in reinforcement learning to hold simulator states. But multiprocessing only provided an implementation of manager called SyncManager who can only do synchronized method calls. This make it not easy to create many managers and make them work in parallel. Fiber implemented an asynchronous version of manager called AsyncManager which allows asynchronous method calls. Users can create many AsyncManager and they can work in parallel. AsyncManager demo \u00b6 We use the example from examples/async_manager.py . In this example, we compare the performance of synchronous and asynchronous managers. We create 4 asynchronous managers and each one of them manages a CartPole-v1 environment. The 4 managers runs in parallel. Each of method call to env is an asynchronous method call. For example, env.step(action) returns a handle object immediately. When you call handle.get() , you will get the actual result from that method call. Download async_manager.py file. This example needs gym and you may install it with: pip install gym Run async manager example with Python: $ python async_manager.py Sync manager took 3.7199201583862305s Async manager took 1.6843571662902832s We should see that asynchronous manager runs much faster than synchronous managers.","title":"More about Fiber"},{"location":"advanced/#using-fiber-with-gpus","text":"Fiber supports running workload with GPUs on Kubernetes. First, you need to configure your Kubernetes cluster for GPU support. Check out this guide for details. Currently, we support Nvidia GPU with NVIDIA GPU device plugin used by GCE . Other types of device and device plugin will be supported in the future. Once GPU is set up on your cluster. We can run some tests to see if GPU is ready for use. Let's create a new docker file called fiber-gpu.docker with the following content: FROM nvidia/cuda:10.0-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y python3-pip RUN pip3 install fiber This time, we use nvidia/cuda:10.0-runtime-ubuntu18.04 as our base image. This is because it already have CUDA libraries properly configured. You can also use other base images, but you may need to configure CUDA path by yourself. Check out this guide for details.","title":"Using Fiber with GPUs"},{"location":"advanced/#using-gpu-with-fiber-master-process","text":"If the master process needs GPU, then it can be done by providing --gpu argument to fiber command when running your program. Alternatively, you can also specify nvidia.com/gpu limits when you launch fiber master process with kubectl . Checkout here for details. Here, we show how this can be done with fiber command. Let's test if GPU is available by running this test command and it should print out the pod name created: $ fiber run --gpu 1 nvidia-smi ... Created pod: fiber-e3ae700b If everything is configured properly, you should see something similar to this when you get logs for that pod: $ kubectl logs fiber-e3ae700b Wed Feb 5 20:33:04 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.67 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:05.0 Off | 0 | | N/A 35C P8 9W / 70W | 0MiB / 15079MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ When we pass --gpu 1 to fiber command, it created a new pod with 1 GPU allocated to that pod. And we can see in our example, one Nvidia Tesla T4 GPU has been allocated to our pod.","title":"Using GPU with Fiber master process"},{"location":"advanced/#using-gpu-with-fiber-child-process","text":"If the child process needs GPU, it can be done with Fiber's resource limits . Simply add @fiber.meta(gpu=x) to your child process function that needs GPU, and Fiber will allocate GPU for your function when it runs. Let's create an simple program with runs a child process with PyTorch and GPU. Create a file called fiber_process_gpu.py . import fiber from fiber import SimpleQueue , Process import torch import numpy as np @fiber . meta ( gpu = 1 ) def process_using_gpu ( q ): input_a , input_b = q . get () a = torch . tensor ( input_a ) . cuda () b = torch . tensor ( input_b ) . cuda () c = torch . matmul ( a , b ) . cpu () . numpy () q . put ( c ) def main (): q = SimpleQueue () p = Process ( target = process_using_gpu , args = ( q ,)) p . start () a = np . random . rand ( 3 , 4 ) b = np . random . rand ( 4 , 2 ) q . put (( a , b )) res = q . get () print ( \"Result is\" , res ) p . terminate () p . join () if __name__ == '__main__' : main () In this example, we have a child process which accepts 2 tensors from the master process, calculate their product with GPU and then return the result to the master process. Because this example needs PyTorch and NumPy, we added a few lines to fiber-gpu.docker . The new docker file looks like this: FROM nvidia/cuda:10.0-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y python3-pip RUN pip3 install --no-cache-dir torch == 1 .2.0 torchvision == 0 .4.0 pillow == 6 .1 RUN pip3 install fiber ADD fiber_process_gpu.py /root/fiber_process_gpu.py Run this program with the following command: $ fiber run python3 /root/fiber_process_gpu.py ... Created pod: fiber-384369de Get logs of pod fiber-384369de . $ kubectl logs fiber-384369de Result is ( array ([[ 0 .10797104, 0 .65835916, 0 .95002519, 0 .83939533 ] , [ 0 .06103808, 0 .39594844, 0 .56635164, 0 .61488279 ] , [ 0 .26484163, 0 .75913394, 0 .45325563, 0 .62634138 ]]) , array ([[ 0 .52567844, 0 .47349188 ] , [ 0 .58966787, 0 .05199646 ] , [ 0 .10254589, 0 .37998549 ] , [ 0 .5943244 , 0 .02409804 ]])) We see that the master process has successfully printed out the product of 2 tensors calculated on GPU from its child process.","title":"Using GPU with Fiber child process"},{"location":"advanced/#working-with-persistent-storage","text":"It is also very important to get your training logs after your training jobs is done. Because the disk space inside running container is not persistent, we need to store logs and outputs of our program on persistent storage. fiber command line tool provided a way to use Kubernetes' PersistentVolumeClaim to storage program output data. Here, we create an NFS Persistent Volume claim following instructions here . # create persistent volume claim on GCP $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/provisioner/nfs-server-gce-pv.yaml # create nfs server $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-server-rc.yaml # create nfs server service $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-server-service.yaml # create persistent volume $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-pv.yaml # create nfs persistent volume claim $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/examples/master/staging/volumes/nfs/nfs-pvc.yaml This will create an NFS persistent volume claim called \"nfs\" with size 1M. Once we have it done, we can try out this feature by: fiber run -v nfs bash -c 'echo \"This file is persisted across runs\" > /persistent/foo.txt' What this command does is in addition to create a new job, it mounted the persistent volume claim nfs to path /persistent inside the container. Then we create a file called /persistent/foo.txt with some content. Then we can retrieve the content of this file from a different job: $ fiber run -v nfs cat /persistent/foo.txt ... Created pod: fiber-88e2197a $ kubectl logs fiber-88e2197a This file is persisted across runs","title":"Working with persistent storage"},{"location":"advanced/#error-handling","text":"Fiber's Pool supports error handling by default. This means when a Pool is created and all the Pool workers are up, if one of the worker crashes in the middle of the computation, then the tasks that was working on by that worker will be retried. Those tasks will be sent to other healthy workers and the master process will not be interrupted by this crash. At the same time, the crashed worker will be cleaned up and a new worker will be started to replace that crashed Pool worker. A Pool with a list of tasks can be viewed as a contract between the process that created the Pool and all worker processes. We define some concepts as below: Task function is a function that is passed to Pool.apply() or other similar functions like Pool.map() as func argument. Task arguments are Python objects that are passed to task functions when they run. Task is a task function combined with task arguments. Task result is the result returned by a task function. Task queue is a queue used internally by a Process Pool to store all tasks. All tasks are distributed to worker processes through task queue. When a user creates a new Pool, a Pool and associated task queue, result queue and pending table are created. All worker processes are also created. First, the Pool will put all tasks into the task queue, which is shared between the master process and worker processes. Each of the workers will fetch a single task (or a batch of tasks) from the task queue as task arguments, then run task function with task arguments. Each time a task is removed from the task queue, an entry in the pending table is added. The entry will have the worker process\u2019 id as its key and the task as its value. Once the worker finished that task, it will put the result in the result queue. And Pool will remove the entry associated with that task from the pending table. If a worker process fails in the middle of processing (Worker 3 in the above diagram). Then its failure will be detected by the Pool which serves as a process manager of all worker processes and constantly checks with Peloton to get all its workers\u2019 states. Then the Pool will put the pending task from the pending table back to the task queue if the previously failed process has a pending task. Then it will start a new worker process (Worker 5) to replace the previously failed process and connect the newly created worker process to a task queue and result queue. In this way, we can make sure that all the tasks that are put into the task queue get process by one of the worker processes. Note that the automatic retry should be only turned on if the task function is idempotent .","title":"Error handling"},{"location":"advanced/#extensions-to-standard-multiprocessing-api","text":"","title":"Extensions to standard multiprocessing API"},{"location":"advanced/#ring","text":"A Ring in fiber stands for a list of processes who work collectively together. Unlike Pool , Ring doesn't have the concept of master process and worker process. All the members inside the Ring shared basically the same responsibility. Each node in the ring usually only talk to it's left and right neighbors. Each member in the Ring has a rank which range from 0 to the number of nodes - 1. Usually ring node 0 has some specially responsibility to gather additional information and usually also serves as a control node. The ring topology is very common in machine learning when doing distributed SGD . Examples include torch.distributed , Horovod , etc. But generally it's really hard to start this kind of workload on a computer cluster, especially when setting up the communication between different nodes are needed. Fiber provides the Ring class to simplify this process. Fiber handles the starting of all the processes and provides network information to the ring-forming framework. The end user only needs to provide an initialization function and the target function that each node runs, and Fiber will handle the rest for the user. An example of using Ring with torch.distributed can be found in examples/ring.py in Fiber's source code repo. As mentioned above, Ring needs a initialization function and a target function. In this example, pytorch_ring_init is the initialization function. In it, we initialize PyTorch and set environment variables to tell PyTorch where is the master node so that all the nodes can be discovered by PyTorch. And pytorch_run_sgd is the target function that does distributed SGD on each of the Ring node. Fiber will first call pytorch_ring_init on each of the node to set up the ring and then call pytorch_run_sgd to do the actual distributed SGD. Fiber Ring . A Fiber Ring with 4 nodes is depicted. Ring node 0 and ring node 3 run on the same machine but in two different containers. Ring nodes 1 and 2 both run on a separate machine. All these processes collectively run a copy of the same function and communicate with each other during the run.","title":"Ring"},{"location":"advanced/#ring-demo","text":"To run everything, we can re-use the docker file from the previous section and download examples/ring.py and put it into the same directory as your docker file. This demo shows how to run a Ring on Kubernetes with GPU. Add a new line to the docker file ADD ring.py /root/ring.py and the new docker file looks like this: FROM nvidia/cuda:10.0-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y python3-pip RUN pip3 install --no-cache-dir torch == 1 .2.0 torchvision == 0 .4.0 pillow == 6 .1 RUN pip3 install fiber ADD fiber_process_gpu.py /root/fiber_process_gpu.py ADD ring.py /root/ring.py Now we run everything with this command: $ fiber run --gpu 1 python3 /root/ring.py ... Created pod: fiber-dbb2d4d9 Check the logs our the job: $ kubectl logs fiber-dbb2d4d9 pytorch ring init, rank 0 2 0 eth0 10 .12.0.116 48707 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz 100 .1%Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz 113 .5%Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz 100 .4%Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz 180 .4%Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw Processing... Done! Rank 0 , epoch 0 : 0 .5177083023861527 Rank 0 , epoch 1 : 0 .12978197557569693 ... This means ring node 0 has been started and running! What about the other nodes in the ring? We can first get the pods that other process are running by: $ kubectl get po | grep process process-3-03e3a1b6 1 /1 Running 0 3m12s In this example, we only have one addition node in the ring and the pod name is process-3-03e3a1b6 . Then we can get it's logs by: $ kubectl logs process-3-03e3a1b6 pytorch ring init, rank 1 2 1 eth0 10 .12.0.116 48707 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz 100 .1%Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz 113 .5%Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz 100 .4%Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz 180 .4%Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw Processing... Done! Rank 1 , epoch 0 : 0 .5106956963933734 Rank 1 , epoch 1 : 0 .12546868318084206 ... It showed we have successfully set up our distributed SGD application with Fiber and PyTorch and GPU!","title":"Ring demo"},{"location":"advanced/#asyncmanager","text":"Python's multiprocessing provides a Manager type to share data between different processes. A manager manages a Python object and it accepts method calls from remote so that the state of the managed object can be changed. The stateful storage nature of Manager make it suitable to be use in reinforcement learning to hold simulator states. But multiprocessing only provided an implementation of manager called SyncManager who can only do synchronized method calls. This make it not easy to create many managers and make them work in parallel. Fiber implemented an asynchronous version of manager called AsyncManager which allows asynchronous method calls. Users can create many AsyncManager and they can work in parallel.","title":"AsyncManager"},{"location":"advanced/#asyncmanager-demo","text":"We use the example from examples/async_manager.py . In this example, we compare the performance of synchronous and asynchronous managers. We create 4 asynchronous managers and each one of them manages a CartPole-v1 environment. The 4 managers runs in parallel. Each of method call to env is an asynchronous method call. For example, env.step(action) returns a handle object immediately. When you call handle.get() , you will get the actual result from that method call. Download async_manager.py file. This example needs gym and you may install it with: pip install gym Run async manager example with Python: $ python async_manager.py Sync manager took 3.7199201583862305s Async manager took 1.6843571662902832s We should see that asynchronous manager runs much faster than synchronous managers.","title":"AsyncManager demo"},{"location":"cli/","text":"fiber.cli fiber.cli This module contains functions for the fiber command line tool. fiber command line tool can be use do to mange the workflow of running jobs on a computer cluster. Check here for an example of how to use this command.","title":"Command Line Tool"},{"location":"config/","text":"fiber.config This module deals with Fiber configurations. There are 3 way of setting Fiber configurations: config file, environment variable and Python code. The priorities are: Python code > environment variable > config file. Config file \u00b6 Fiber config file is a plain text file following Python's configparser file format. It needs to be named .fiberconfig and put into the directory where you launch your code. An example .fiberconfig file: [default] log_level=debug log_file=stdout backend=local Environment variable \u00b6 Alternatively, you can also use environment variables to pass configurations to Fiber. The environment variable names are in format FIBER_ + config name in upper case. For example, an equivalent way of specifying the above config using environment variables is: FIBER_LOG_LEVEL=debug FIBER_LOG_FILE=stdout FIBER_BACKEND=local python code.py ... Python code \u00b6 You can also set Fiber config in your Python code: import fiber.config as fiber_config ... def main (): fiber_config . log_level = \"debug\" fiber_config . log_file = \"stdout\" fiber_config . backend = \"local\" Note that almost all of the configurations needs to be set before you launch any Fiber processes. Config Config ( self , conf_file = None ) Fiber configuration object. Available configurations: key Type Default Notes debug bool False Set this to True to turn on debugging image str None Docker image to use when starting new processes default_image str None Default docker image to use when image config value is not set backend str None Fiber backend to use when starting new processes. Check here for available backends default_backend str local Default Fiber backend to use when backend config is not set log_level str/int logging.INFO Fiber log level. This config accepts either a int value (log levels from logging module like logging.INFO ) or strings: debug , info , warning , error , critical log_file str /tmp/fiber.log Default fiber log file path. Fiber will append the process name to this value and create one log file for each process. A special value stdout means to print the logs to standard output ipc_admin_master_port int 0 The port that master process uses to communicate with child processes. Default value is 0 which means the master process will choose a random port kubernetes_namespace str default The namespace that Fiber kubernetes backend will use to create pods and do other work on Kubernetes","title":"Config"},{"location":"config/#config-file","text":"Fiber config file is a plain text file following Python's configparser file format. It needs to be named .fiberconfig and put into the directory where you launch your code. An example .fiberconfig file: [default] log_level=debug log_file=stdout backend=local","title":"Config file"},{"location":"config/#environment-variable","text":"Alternatively, you can also use environment variables to pass configurations to Fiber. The environment variable names are in format FIBER_ + config name in upper case. For example, an equivalent way of specifying the above config using environment variables is: FIBER_LOG_LEVEL=debug FIBER_LOG_FILE=stdout FIBER_BACKEND=local python code.py ...","title":"Environment variable"},{"location":"config/#python-code","text":"You can also set Fiber config in your Python code: import fiber.config as fiber_config ... def main (): fiber_config . log_level = \"debug\" fiber_config . log_file = \"stdout\" fiber_config . backend = \"local\" Note that almost all of the configurations needs to be set before you launch any Fiber processes.","title":"Python code"},{"location":"context/","text":"fiber.context FiberContext FiberContext () Manager FiberContext . Manager () Returns a manager associated with a running server process The managers methods such as Lock() , Condition() and Queue() can be used to create shared objects. Pool FiberContext . Pool ( processes = None , initializer = None , initargs = (), maxtasksperchild = None , error_handling = False ) Returns a process pool object SimpleQueue FiberContext . SimpleQueue () Returns a queue object active_children FiberContext . active_children () Get a list of children processes of the current process. Returns : A list of children processes. Example: p = fiber . Process ( target = time . sleep , args = ( 10 ,)) p . start () print ( fiber . active_children ()) Pipe FiberContext . Pipe ( duplex = True ) Returns two connection object connected by a pipe current_process FiberContext . current_process () Return a Process object representing the current process. Example: print ( fiber . current_process ()) Process FiberContext . Process ( group = None , target = None , name = None , args = (), kwargs = {}, * , daemon ) Create and manage Fiber processes. The API is compatible with Python's multiprocessing.Process, check here for multiprocessing's documents. Example usage: p = Process ( target = f , args = ( 'Fiber' ,)) p . start () pid \u00b6 Process.pid Return the current process PID. If the process hasn't been fully started, the value will be None . This PID is assigned by Fiber and is different from the operating system process PID. The value of pid is derived from the job ID of the underlying job that runs this Fiber process. name \u00b6 Process.name Return the name of this Fiber process. This value need to be set before the start of the Process. Example: p = fiber . Process ( target = print , args = ( \"demo\" )) p . name = \"DemoProcess\" daemon \u00b6 Process.daemon In multiprocessing, if a process has daemon set, it will be terminated when it's parent process exits. In Fiber, current this value has no effect, all processes will be cleaned up when their parent exits. authkey \u00b6 Process.authkey Authkey is used to authenticate between parent and child processes. It is a byte string. exitcode \u00b6 Process.exitcode The exit code of current process. If the process has not exited, the exitcode is None . If the current process has exited, the value of this is an integer. sentinel \u00b6 Process.sentinel Returns a file descriptor that becomes \"ready\" when the process exits. You can call select and other eligible functions that works on fds on this file descriptor.","title":"Context"},{"location":"context/#pid","text":"Process.pid Return the current process PID. If the process hasn't been fully started, the value will be None . This PID is assigned by Fiber and is different from the operating system process PID. The value of pid is derived from the job ID of the underlying job that runs this Fiber process.","title":"pid"},{"location":"context/#name","text":"Process.name Return the name of this Fiber process. This value need to be set before the start of the Process. Example: p = fiber . Process ( target = print , args = ( \"demo\" )) p . name = \"DemoProcess\"","title":"name"},{"location":"context/#daemon","text":"Process.daemon In multiprocessing, if a process has daemon set, it will be terminated when it's parent process exits. In Fiber, current this value has no effect, all processes will be cleaned up when their parent exits.","title":"daemon"},{"location":"context/#authkey","text":"Process.authkey Authkey is used to authenticate between parent and child processes. It is a byte string.","title":"authkey"},{"location":"context/#exitcode","text":"Process.exitcode The exit code of current process. If the process has not exited, the exitcode is None . If the current process has exited, the value of this is an integer.","title":"exitcode"},{"location":"context/#sentinel","text":"Process.sentinel Returns a file descriptor that becomes \"ready\" when the process exits. You can call select and other eligible functions that works on fds on this file descriptor.","title":"sentinel"},{"location":"core/","text":"fiber.core ProcessStatus ProcessStatus ( cls , value , names = None , * , module , qualname , type , start ) An enumeration. Backend Backend ()","title":"Core"},{"location":"examples/","text":"Run OpenAI Baselines on Kubernetes with Fiber \u00b6 In this example, we'll show you how to integrate fiber with OpenAI baselines with just one line of code change. If your project is already using Python's multiprocessing, then integrate it with Fiber is very easy. Here, we are going to use OpenAI Baselines as an example to show how to easily run code written with multiprocessing on Kubernetes easily. Prepare the code \u00b6 First, we clone baselines from Github, create a new branch and setup our local environment: git clone https://github.com/openai/baselines cd baselines git checkout -b fiber virtualenv -p python3 env . env/bin/activate echo \"env\" > .dockerignore pip install \"tensorflow<2\" pip install -e . pip install fiber Test that if the environment works: python -m baselines.run --alg=ppo2 --env=CartPole-v0 --network=mlp --num_timesteps=10000 If it works, you should see something like this in your output --------------------------------------- | eplenmean | 23.8 | | eprewmean | 23.8 | | fps | 1.95e+03 | | loss/approxkl | 0.000232 | | loss/clipfrac | 0 | | loss/policy_entropy | 0.693 | | loss/policy_loss | -0.00224 | | loss/value_loss | 48.4 | | misc/explained_variance | -0.000784 | | misc/nupdates | 1 | | misc/serial_timesteps | 2.05e+03 | | misc/time_elapsed | 1.05 | | misc/total_timesteps | 2.05e+03 | --------------------------------------- OpenAI baselines has a SubprocVecEnv that, according to it's documentation, runs multiple environments in parallel in subprocesses and communicates with them via pipes. We'll start from here to modify it to work with Fiber: Fiberization (Or adapt your code to run with Fiber) \u00b6 Open baselines/common/vec_env/subproc_vec_env.py and change this line: import multiprocessing as mp to import fiber as mp Let's do a quick test to see if this change works python -m baselines.run --alg = ppo2 --env = CartPole-v0 --network = mlp --num_timesteps = 10000 --num_env 2 We set --num_env 2 to make sure baselines is using SubprocVecEnv . If everything works, we should see similar output as the previous run. Containerize the application \u00b6 OpenAI baselines already has a Dockerfile available, so we just need to add fiber to it by adding a line RUN pip install fiber . After modification, the Dockerfile looks like this: FROM python:3.6 RUN apt-get -y update && apt-get -y install ffmpeg # RUN apt-get -y update && apt-get -y install git wget python-dev python3-dev libopenmpi-dev python-pip zlib1g-dev cmake python-opencv ENV CODE_DIR /root/code COPY . $CODE_DIR /baselines WORKDIR $CODE_DIR/baselines # Clean up pycache and pyc files RUN rm -rf __pycache__ && \\ find . -name \"*.pyc\" -delete && \\ pip install 'tensorflow < 2' && \\ pip install -e . [ test ] RUN pip install fiber CMD /bin/bash It's a good habit to make sure everything works locally before submitting the job to the bigger cluster because this will save you a lot of debugging time. So we build our docker image locally: docker build -t fiber-openai-baselines . When Fiber starts new dockers locally, it will mount your home directory into docker. So we need to modify baselines' log dir to make sure it can write logs to the correct place by adding an argument --log_path=logs . By default, baselines writes to /tmp dir which is not shared by Fiber master process and subprocesses. We also add --num_env 2 to make sure baselines uses SubprocVecEnv so that Fiber processes can be launched. FIBER_BACKEND = docker FIBER_IMAGE = fiber-openai-baselines:latest python -m baselines.run --alg = ppo2 --env = CartPole-v0 --network = mlp --num_timesteps = 10000 --num_env 2 --log_path = logs Running on Kubernetes \u00b6 Now let's run our fiberized OpenAI baselines on Kubernetes. This time we run 1e7 time steps. Also, we want to store the output of the run on persistent storage. We can do this with fiber command's mounting persistent volumes feature. $ fiber run -v fiber-pv-claim python -m baselines.run --alg = ppo2 --env = CartPole-v0 --network = mlp --num_timesteps = 1e7 --num_env 2 --log_path = /persistent/baselines/logs/ It should output something like this: Created pod: baselines-d00eb2ef After the job is done, you can copy the logs with these commands: $ fiber cp fiber-pv-claim:/persistent/baselines/logs baselines-logs","title":"Examples"},{"location":"examples/#run-openai-baselines-on-kubernetes-with-fiber","text":"In this example, we'll show you how to integrate fiber with OpenAI baselines with just one line of code change. If your project is already using Python's multiprocessing, then integrate it with Fiber is very easy. Here, we are going to use OpenAI Baselines as an example to show how to easily run code written with multiprocessing on Kubernetes easily.","title":"Run OpenAI Baselines on Kubernetes with Fiber"},{"location":"examples/#prepare-the-code","text":"First, we clone baselines from Github, create a new branch and setup our local environment: git clone https://github.com/openai/baselines cd baselines git checkout -b fiber virtualenv -p python3 env . env/bin/activate echo \"env\" > .dockerignore pip install \"tensorflow<2\" pip install -e . pip install fiber Test that if the environment works: python -m baselines.run --alg=ppo2 --env=CartPole-v0 --network=mlp --num_timesteps=10000 If it works, you should see something like this in your output --------------------------------------- | eplenmean | 23.8 | | eprewmean | 23.8 | | fps | 1.95e+03 | | loss/approxkl | 0.000232 | | loss/clipfrac | 0 | | loss/policy_entropy | 0.693 | | loss/policy_loss | -0.00224 | | loss/value_loss | 48.4 | | misc/explained_variance | -0.000784 | | misc/nupdates | 1 | | misc/serial_timesteps | 2.05e+03 | | misc/time_elapsed | 1.05 | | misc/total_timesteps | 2.05e+03 | --------------------------------------- OpenAI baselines has a SubprocVecEnv that, according to it's documentation, runs multiple environments in parallel in subprocesses and communicates with them via pipes. We'll start from here to modify it to work with Fiber:","title":"Prepare the code"},{"location":"examples/#fiberization-or-adapt-your-code-to-run-with-fiber","text":"Open baselines/common/vec_env/subproc_vec_env.py and change this line: import multiprocessing as mp to import fiber as mp Let's do a quick test to see if this change works python -m baselines.run --alg = ppo2 --env = CartPole-v0 --network = mlp --num_timesteps = 10000 --num_env 2 We set --num_env 2 to make sure baselines is using SubprocVecEnv . If everything works, we should see similar output as the previous run.","title":"Fiberization (Or adapt your code to run with Fiber)"},{"location":"examples/#containerize-the-application","text":"OpenAI baselines already has a Dockerfile available, so we just need to add fiber to it by adding a line RUN pip install fiber . After modification, the Dockerfile looks like this: FROM python:3.6 RUN apt-get -y update && apt-get -y install ffmpeg # RUN apt-get -y update && apt-get -y install git wget python-dev python3-dev libopenmpi-dev python-pip zlib1g-dev cmake python-opencv ENV CODE_DIR /root/code COPY . $CODE_DIR /baselines WORKDIR $CODE_DIR/baselines # Clean up pycache and pyc files RUN rm -rf __pycache__ && \\ find . -name \"*.pyc\" -delete && \\ pip install 'tensorflow < 2' && \\ pip install -e . [ test ] RUN pip install fiber CMD /bin/bash It's a good habit to make sure everything works locally before submitting the job to the bigger cluster because this will save you a lot of debugging time. So we build our docker image locally: docker build -t fiber-openai-baselines . When Fiber starts new dockers locally, it will mount your home directory into docker. So we need to modify baselines' log dir to make sure it can write logs to the correct place by adding an argument --log_path=logs . By default, baselines writes to /tmp dir which is not shared by Fiber master process and subprocesses. We also add --num_env 2 to make sure baselines uses SubprocVecEnv so that Fiber processes can be launched. FIBER_BACKEND = docker FIBER_IMAGE = fiber-openai-baselines:latest python -m baselines.run --alg = ppo2 --env = CartPole-v0 --network = mlp --num_timesteps = 10000 --num_env 2 --log_path = logs","title":"Containerize the application"},{"location":"examples/#running-on-kubernetes","text":"Now let's run our fiberized OpenAI baselines on Kubernetes. This time we run 1e7 time steps. Also, we want to store the output of the run on persistent storage. We can do this with fiber command's mounting persistent volumes feature. $ fiber run -v fiber-pv-claim python -m baselines.run --alg = ppo2 --env = CartPole-v0 --network = mlp --num_timesteps = 1e7 --num_env 2 --log_path = /persistent/baselines/logs/ It should output something like this: Created pod: baselines-d00eb2ef After the job is done, you can copy the logs with these commands: $ fiber cp fiber-pv-claim:/persistent/baselines/logs baselines-logs","title":"Running on Kubernetes"},{"location":"getting-started/","text":"In this guide, we will walk through the basic features of Fiber. By the end of this guide, you should be able to launch your own Fiber application on a Kubernetes cluster! A minimal example \u00b6 If you have already installed Fiber on your computer and your environment is supported by Fiber , then we can get things started. Open your favorite editor and create a Python file called hello_fiber.py with the following content: import fiber if __name__ == '__main__' : fiber . Process ( target = print , args = ( 'Hello, Fiber!' ,)) . start () You may find that the API is the same as Python's multiprocessing library. In fact, most of multiprocessing's API is supported by Fiber. You can take an program that is written with multiprocessing and changes a few lines to make it work with Fiber. We will see some examples later. Run it with the following command: python hello_fiber.py You should see the following output: Hello, Fiber! Under the hood, what Fiber does is that it launches a new process locally on your computer, and then run print function with arguments 'Hello, Fiber!' . \"Isn't this just multiprocessing?\", you may ask. Indeed, Fiber works the same as multiprocessing when running locally on a single computer. But we will show how this simple design can be powerful when you run things on a computer cluster. A more complex example \u00b6 The previous example is too easy, isn't it? Let's do something more complex. In this example, we will create a simple program that estimates Pi with Monte Carlo Method. Create a new file called pi_estimation.py with the following content: from fiber import Pool import random NUM_SAMPLES = int ( 1e6 ) def is_inside ( p ): x , y = random . random (), random . random () return x * x + y * y < 1 def main (): pool = Pool ( processes = 4 ) pi = 4.0 * sum ( pool . map ( is_inside , range ( 0 , NUM_SAMPLES ))) / NUM_SAMPLES print ( \"Pi is roughly {} \" . format ( pi )) if __name__ == '__main__' : main () In this example, we use Monte Carlo method to estimate the value of Pi. Run it with the following command: python pi_estimation.py You should see something like this: Pi is roughly 3.140636 What Fiber does is it created a pool of 4 workers, pass all the workload to them and collect results from them. In this example, each worker calculates whether a single random point is inside a circle or not. And we can increase the degree of degree of parallelism by increasing the number of Pool workers. Containerize your program \u00b6 Before we can run our program on a computer cluster, we need to first encapsulate our computation environment in a container . It is a method of virtualization that package an application's code and dependencies into a single object. The aim is to allow application to run reliably and consistently from one environment to another environment. For Fiber, the benefits of using a container includes: container is portable and we can run it locally and on a remote cluster container encapsulates the running environment of the program, and this makes sure that if something works locally, it most likely will also work on a computer cluster. This step requires docker , check out here to see how it can be installed on your system. To encapsulate our Pi estimation program, we create a file called Dockerfile for it. # Dockerfile FROM python:3.6-buster ADD pi_estimation.py /root/pi_estimation.py RUN pip install fiber Make sure your current directory doesn't include anything irreverent or you can ignore them from the build by putting them in a .dockerignore file. Build an image by running the following command: docker build -t fiber-pi-estimation . After the image is built, you will get an docker image called fiber-pi-estimation:latest . Test your container \u00b6 With this image built, you can test if your program works inside the docker by running it with Fiber's docker backend. Fiber provides many different backends for different running environment. Checkout here for details. You can run the following command to test if your recently build container works or not by running the following command: FIBER_BACKEND = docker FIBER_IMAGE = fiber-pi-estimation:latest python pi_estimation.py You should see the familiar output and it looks like: Pi is roughly 3.142324 Fiber config \u00b6 So what's the difference between this one and the previous run? In this run, we use special environment variables to tell Fiber what backend to use and what docker image to use. These environment variables are a part of Fiber's configuration system. FIBER_BACKEND tells Fiber what backend to use. Currently, Fiber supports these backends: local , docker and kubernetes . When FIBER_BACKEND is set to docker , all new processes will be launched through docker backend which means all of them will be running inside their own docker container. FIBER_IMAGE tells Fiber what docker image to use when launching new containers. This container provides the running environment for your child processes, so it needs to have Fiber installed in it. And we already did that in the previous step when building the docker container. Note that in this example, the master process (the one you started with python pi_estimation.py ) still runs on local machine instead of inside a docker container. All the processes started by Fiber are inside containers. You can checkout the containers launched by Fiber by running: docker ps -a|grep fiber-pi-estimation Alternatively, you can also create a .fiberconfig file to pass the configurations to Fiber. The equivalent config file is: #.fiberconfig backend=docker image=fiber-pi-estimation To learn more about Fiber's configuration system, check out here . Running on a computer cluster \u00b6 With all the previous steps finished, now it's time to try some distributed computing on a real computer cluster. The good thing is that most of the work has already been done by now. Here we use Google Kubernetes Engine on Google Cloud as an example. You'll need to install Google Cloud SDK and kubectl on your machine. Also, you need to authenticate docker to access Google Container Registry (GCR) following this guide . We first config the cluster to grant permission to the default service account so that Fiber can access Kubernetes API from within the cluster. kubectl apply -f https://raw.githubusercontent.com/uber/fiber/master/configs/rbac.yaml Then we tag our image and push it to a container registry that is accessible by your Kubernetes cluster. docker tag fiber-pi-estimation:latest gcr.io/ [ your-project-name ] /fiber-pi-estimation:latest docker push gcr.io/ [ your-project-name ] /fiber-pi-estimation:latest Now that the docker image is available, we can launch our job by: kubectl create job fiber-pi-estimation --image = gcr.io/ [ your-project-name ] /fiber-pi-estimation:latest -- python3 /root/pi_estimation.py We should see something like this in the output: job.batch/fiber-pi-estimation created The job has been submitted to Kubernetes cluster, and now we can get its logs. It may take some time before the job is scheduled. kubectl logs $( kubectl get po | grep fiber-pi-estimation | awk '{print $1}' ) And you should see this familiar output from the above command: Pi is roughly 3.139972 Congratulations! You have successfully run your first Fiber program on Kubernetes. On Kubernetes, Fiber behaves similarly to when running locally with Docker. Each process becomes a Kubernetes pod and all the pods work collectively to compute our estimation of Pi! Running with fiber command \u00b6 If the above process looks too complex for you, we have a better solution. To simplify the workflow of running jobs on Kubernetes, we create a command-line tool named fiber to help to manage the job running process. Specifically, fiber run command can help you build docker images, push images to your GCR and run jobs for you. It currently only works for Google Cloud but we plan to extend it to work with other platforms. With fiber command, running a fiber program on Kubernetes is very simple. fiber run looks for a valid docker file on the current directory, builds a docker image with that, tag it and push it to GCR with your default google cloud project name, and then run the command that you passed to fiber run . We will re-use our previously written Dockerfile . Instead of building, pushing images and launch jobs by our own, now everything can be simplified to one command: fiber run python3 /root/pi_estimation.py Note that we use /root/pi_estimation.py is the path of the source file inside docker image. We should see something like this in the last line of the output: Created pod: fiber-16fb282d And fiber-16fb282d is the Kubernetes pod that we created for our main process. To get the output of our job, simply run: kubectl logs fiber-16fb282d And we should see the familiar output: Pi is roughly 3.141044 For detailed docs on fiber command, check out here . Resource Limits \u00b6 Now that we can run our program on a computer cluster, it's important to think about how much resources each of the processes is going to use so that all the jobs can be properly scheduled. We can specify how many CPU cores, how much memory and how many GPU one process needs by using fiber.meta API. Take our Pi estimation program as an example. Because our is_inside can only use one CPU and doesn't need much memory, we can specify these resource limits by using @fiber.meta decorator. A modified version of pi_estimation.py looks like this: import fiber from fiber import Pool import random NUM_SAMPLES = int ( 1e6 ) # 1 CPU core and 1000M of memory @fiber . meta ( cpu = 1 , memory = 1000 ) def is_inside ( p ): x , y = random . random (), random . random () return x * x + y * y < 1 def main (): pool = Pool ( processes = 4 ) pi = 4.0 * sum ( pool . map ( is_inside , range ( 0 , NUM_SAMPLES ))) / NUM_SAMPLES print ( \"Pi is roughly {} \" . format ( pi )) if __name__ == '__main__' : main () We now run this new version with fiber run command: $ fiber run python3 /root/pi_estimation.py ... Created pod: fiber-36c2da03 And then we get the logs: $ kubectl logs fiber-36c2da03 Pi is roughly 3.141004","title":"Getting Started"},{"location":"getting-started/#a-minimal-example","text":"If you have already installed Fiber on your computer and your environment is supported by Fiber , then we can get things started. Open your favorite editor and create a Python file called hello_fiber.py with the following content: import fiber if __name__ == '__main__' : fiber . Process ( target = print , args = ( 'Hello, Fiber!' ,)) . start () You may find that the API is the same as Python's multiprocessing library. In fact, most of multiprocessing's API is supported by Fiber. You can take an program that is written with multiprocessing and changes a few lines to make it work with Fiber. We will see some examples later. Run it with the following command: python hello_fiber.py You should see the following output: Hello, Fiber! Under the hood, what Fiber does is that it launches a new process locally on your computer, and then run print function with arguments 'Hello, Fiber!' . \"Isn't this just multiprocessing?\", you may ask. Indeed, Fiber works the same as multiprocessing when running locally on a single computer. But we will show how this simple design can be powerful when you run things on a computer cluster.","title":"A minimal example"},{"location":"getting-started/#a-more-complex-example","text":"The previous example is too easy, isn't it? Let's do something more complex. In this example, we will create a simple program that estimates Pi with Monte Carlo Method. Create a new file called pi_estimation.py with the following content: from fiber import Pool import random NUM_SAMPLES = int ( 1e6 ) def is_inside ( p ): x , y = random . random (), random . random () return x * x + y * y < 1 def main (): pool = Pool ( processes = 4 ) pi = 4.0 * sum ( pool . map ( is_inside , range ( 0 , NUM_SAMPLES ))) / NUM_SAMPLES print ( \"Pi is roughly {} \" . format ( pi )) if __name__ == '__main__' : main () In this example, we use Monte Carlo method to estimate the value of Pi. Run it with the following command: python pi_estimation.py You should see something like this: Pi is roughly 3.140636 What Fiber does is it created a pool of 4 workers, pass all the workload to them and collect results from them. In this example, each worker calculates whether a single random point is inside a circle or not. And we can increase the degree of degree of parallelism by increasing the number of Pool workers.","title":"A more complex example"},{"location":"getting-started/#containerize-your-program","text":"Before we can run our program on a computer cluster, we need to first encapsulate our computation environment in a container . It is a method of virtualization that package an application's code and dependencies into a single object. The aim is to allow application to run reliably and consistently from one environment to another environment. For Fiber, the benefits of using a container includes: container is portable and we can run it locally and on a remote cluster container encapsulates the running environment of the program, and this makes sure that if something works locally, it most likely will also work on a computer cluster. This step requires docker , check out here to see how it can be installed on your system. To encapsulate our Pi estimation program, we create a file called Dockerfile for it. # Dockerfile FROM python:3.6-buster ADD pi_estimation.py /root/pi_estimation.py RUN pip install fiber Make sure your current directory doesn't include anything irreverent or you can ignore them from the build by putting them in a .dockerignore file. Build an image by running the following command: docker build -t fiber-pi-estimation . After the image is built, you will get an docker image called fiber-pi-estimation:latest .","title":"Containerize your program"},{"location":"getting-started/#test-your-container","text":"With this image built, you can test if your program works inside the docker by running it with Fiber's docker backend. Fiber provides many different backends for different running environment. Checkout here for details. You can run the following command to test if your recently build container works or not by running the following command: FIBER_BACKEND = docker FIBER_IMAGE = fiber-pi-estimation:latest python pi_estimation.py You should see the familiar output and it looks like: Pi is roughly 3.142324","title":"Test your container"},{"location":"getting-started/#fiber-config","text":"So what's the difference between this one and the previous run? In this run, we use special environment variables to tell Fiber what backend to use and what docker image to use. These environment variables are a part of Fiber's configuration system. FIBER_BACKEND tells Fiber what backend to use. Currently, Fiber supports these backends: local , docker and kubernetes . When FIBER_BACKEND is set to docker , all new processes will be launched through docker backend which means all of them will be running inside their own docker container. FIBER_IMAGE tells Fiber what docker image to use when launching new containers. This container provides the running environment for your child processes, so it needs to have Fiber installed in it. And we already did that in the previous step when building the docker container. Note that in this example, the master process (the one you started with python pi_estimation.py ) still runs on local machine instead of inside a docker container. All the processes started by Fiber are inside containers. You can checkout the containers launched by Fiber by running: docker ps -a|grep fiber-pi-estimation Alternatively, you can also create a .fiberconfig file to pass the configurations to Fiber. The equivalent config file is: #.fiberconfig backend=docker image=fiber-pi-estimation To learn more about Fiber's configuration system, check out here .","title":"Fiber config"},{"location":"getting-started/#running-on-a-computer-cluster","text":"With all the previous steps finished, now it's time to try some distributed computing on a real computer cluster. The good thing is that most of the work has already been done by now. Here we use Google Kubernetes Engine on Google Cloud as an example. You'll need to install Google Cloud SDK and kubectl on your machine. Also, you need to authenticate docker to access Google Container Registry (GCR) following this guide . We first config the cluster to grant permission to the default service account so that Fiber can access Kubernetes API from within the cluster. kubectl apply -f https://raw.githubusercontent.com/uber/fiber/master/configs/rbac.yaml Then we tag our image and push it to a container registry that is accessible by your Kubernetes cluster. docker tag fiber-pi-estimation:latest gcr.io/ [ your-project-name ] /fiber-pi-estimation:latest docker push gcr.io/ [ your-project-name ] /fiber-pi-estimation:latest Now that the docker image is available, we can launch our job by: kubectl create job fiber-pi-estimation --image = gcr.io/ [ your-project-name ] /fiber-pi-estimation:latest -- python3 /root/pi_estimation.py We should see something like this in the output: job.batch/fiber-pi-estimation created The job has been submitted to Kubernetes cluster, and now we can get its logs. It may take some time before the job is scheduled. kubectl logs $( kubectl get po | grep fiber-pi-estimation | awk '{print $1}' ) And you should see this familiar output from the above command: Pi is roughly 3.139972 Congratulations! You have successfully run your first Fiber program on Kubernetes. On Kubernetes, Fiber behaves similarly to when running locally with Docker. Each process becomes a Kubernetes pod and all the pods work collectively to compute our estimation of Pi!","title":"Running on a computer cluster"},{"location":"getting-started/#running-with-fiber-command","text":"If the above process looks too complex for you, we have a better solution. To simplify the workflow of running jobs on Kubernetes, we create a command-line tool named fiber to help to manage the job running process. Specifically, fiber run command can help you build docker images, push images to your GCR and run jobs for you. It currently only works for Google Cloud but we plan to extend it to work with other platforms. With fiber command, running a fiber program on Kubernetes is very simple. fiber run looks for a valid docker file on the current directory, builds a docker image with that, tag it and push it to GCR with your default google cloud project name, and then run the command that you passed to fiber run . We will re-use our previously written Dockerfile . Instead of building, pushing images and launch jobs by our own, now everything can be simplified to one command: fiber run python3 /root/pi_estimation.py Note that we use /root/pi_estimation.py is the path of the source file inside docker image. We should see something like this in the last line of the output: Created pod: fiber-16fb282d And fiber-16fb282d is the Kubernetes pod that we created for our main process. To get the output of our job, simply run: kubectl logs fiber-16fb282d And we should see the familiar output: Pi is roughly 3.141044 For detailed docs on fiber command, check out here .","title":"Running with fiber command"},{"location":"getting-started/#resource-limits","text":"Now that we can run our program on a computer cluster, it's important to think about how much resources each of the processes is going to use so that all the jobs can be properly scheduled. We can specify how many CPU cores, how much memory and how many GPU one process needs by using fiber.meta API. Take our Pi estimation program as an example. Because our is_inside can only use one CPU and doesn't need much memory, we can specify these resource limits by using @fiber.meta decorator. A modified version of pi_estimation.py looks like this: import fiber from fiber import Pool import random NUM_SAMPLES = int ( 1e6 ) # 1 CPU core and 1000M of memory @fiber . meta ( cpu = 1 , memory = 1000 ) def is_inside ( p ): x , y = random . random (), random . random () return x * x + y * y < 1 def main (): pool = Pool ( processes = 4 ) pi = 4.0 * sum ( pool . map ( is_inside , range ( 0 , NUM_SAMPLES ))) / NUM_SAMPLES print ( \"Pi is roughly {} \" . format ( pi )) if __name__ == '__main__' : main () We now run this new version with fiber run command: $ fiber run python3 /root/pi_estimation.py ... Created pod: fiber-36c2da03 And then we get the logs: $ kubectl logs fiber-36c2da03 Pi is roughly 3.141004","title":"Resource Limits"},{"location":"installation/","text":"To install Fiber: pip install fiber","title":"Installation"},{"location":"introduction/","text":"Distributed Computing for AI Made Simple Jiale Zhi, Rui Wang, Jeff Clune and Kenneth O. Stanley Fiber - Distributed Computing for AI Made Simple. Icon by Flat UI Kit . Project Homepage: GitHub Recent advances in machine learning are consistently enabled by increasing amounts of computation . More and more algorithms exploit parallelism and rely on distributed training for processing an enormous amount of data. Both the need for more data and more training impose great challenges on the software that manages and utilizes the large scale computational resource. Within Uber, we've developed algorithms like POET , Go-Explore , GTN , etc., that leverage a large amount of computation. To enable future generations of large-scale computation for algorithms like these, we have developed a new system called Fiber that helps users scale what might otherwise be only local computation to hundreds or even thousands of machines with ease. The challenge of large-scale distributed computation \u00b6 In an ideal world, scaling an application that runs on one machine to an application that runs on a cluster of machines should be as easy as changing a command-line argument. However, that is not an easy task in the real world. While working with many people who run large scale distributed computing jobs on a daily basis, we found that there are several reasons why it is so hard to harness distributed computing nowadays: There is a huge gap between making code work locally on laptops or desktops and running code on a production cluster. You can make MPI work locally but it's a completely different process to run it on a computer cluster. No dynamic scaling is available. If you launch a job that requires a large amount of resources, then most likely you'll need to wait until everything is allocated before you can run your job. This waiting to scale up makes it less efficient. Error handling is missing. While running, some jobs may fail. And you may be put into a very nasty situation where you have to recover part of the result or discard the whole run. High learning cost. Each system has different APIs and conventions for programming. To launch jobs with a new system, a user has to learn a set of completely new conventions before jobs can be launched. The new Fiber platform addresses each of these issues explicitly, potentially opening up seamless large-scale distributed computing to a much wider population of users. Introducing Fiber \u00b6 Fiber is a Python-based distributed computing library for modern computer clusters. Instead of programming your desktop or laptop, now you can program the whole computer cluster. Originally, it was developed to power large scale parallel scientific computation projects like POET and it has been used to power similar projects within Uber. The key features of Fiber include: Easy to use. Fiber allows you to write programs that run on a computer cluster without the need to dive into the details of the computer cluster. Easy to learn. Fiber provides the same API as Python's standard multiprocessing library that people are familiar with. If you know how to use multiprocessing, you can program a computer cluster with Fiber. Fast performance. Fiber's communication backbone is built on top of Nanomsg , which is a high-performance asynchronous messaging library to allow fast and reliable communication. No need for deployment. You run Fiber application the same way as running a normal application on a computer cluster and Fiber handles the rest for you. Reliable computation. Fiber has built-in error handling when you are running a pool of workers. Users can focus on writing the actual application code instead of dealing with crashed workers. In addition, Fiber can be used together with other specialized frameworks in areas where performance is critical. Examples include distributed SGD where many existing frameworks like Horovod or torch.distributed have already provided very good solutions. Fiber can be used together with such platforms by using Fiber's Ring feature to help to set up a distributed training job on computer clusters. Figure 1: Fiber overview. The diagram shows how Fiber works on a computer cluster. It starts many different job-backed-processes and runs different Fiber components and user processes inside them. Fiber Master is the main process that manages all the other processes. Some processes like Ring Node maintain communications between each member. Fiber can (1) help users who are working on large-scale distributed computing to reduce the time to go from ideas to actually running distributed jobs on computation clusters, (2) shield users from details of configuration and resource allocation tasks, (3) enable faster debug cycles, and (4) simplify the transition from local to cluster development. Architecture \u00b6 Fiber bridges the classical multiprocessing API with a flexible selection of backends that can run on different cluster management systems. To achieve this integration, Fiber is split into three different layers: the API layer , backend layer and cluster layer . The API layer provides basic building blocks for Fiber like processes, queues, pools and managers. They have the same semantics as in multiprocessing, but are extended to work in distributed environments. The backend layer handles tasks like creating or terminating jobs on different cluster managers. When a new backend is added, all the other Fiber components (queues, pools, etc.) do not need to be changed. Finally, the cluster layer consists of different cluster managers. Although they are not a part of Fiber itself, they help Fiber to manage resources and keep track of different jobs, thereby reducing the number of items that Fiber needs to track. This overall architecture is summarized in figure 2. Figure 2: Fiber architecture. Job-Backed Process \u00b6 Fiber introduces a new concept called job-backed processes (also called a Fiber process ). It is similar to the process in Python's multiprocessing library, but more flexible: while a process in multiprocessing only runs on a local machine, a Fiber process can run remotely on a different machine or locally on the same machine. When starting a new Fiber process, Fiber creates a new job with the proper Fiber backend on the current computer cluster. Figure 3: Job-backed processes. Each job-backed process is a containerized job running on the computer cluster. Each job-backed process will also have its own allocation of CPU, GPU and other types of resources. The code that runs inside the container is self-contained. Fiber uses containers to encapsulate the running environment of current processes, including all the required files, input data, other dependent program packages, etc., to ensure everything is self-contained. All the child processes are started with the same container image as the parent process to guarantee a consistent running environment. Because each process is a cluster job, its life cycle is the same as any job on the cluster. To make it easy for users, Fiber is designed to directly interact with computer cluster managers. Because of this, Fiber doesn't need to be set up on multiple machines or bootstrapped by any other mechanisms, unlike Spark or IPyParallel. It only needs to be installed on a single machine as a normal Python pip package. Components \u00b6 Fiber implements most multiprocessing APIs on top of Fiber processes including pipes, queues, pools, and managers. Queues and pipes in Fiber behave the same as in multiprocessing. The difference is that queues and pipes are now shared by multiple processes running on different machines. Two processes can read from and write to the same pipe. Furthermore, queues can be shared among many processes on different machines and each process can send to or receive from the same queue at the same time. Fiber's queue is implemented with Nanomsg, a high-performance asynchronous message queue system. Figure 4: Fiber Queue. This diagram shows a Fiber queue shared across three different Fiber processes. One Fiber process is located on the same machine as the queue and the other two processes are located on another machine. One process is writing to the queue and the other two are reading from the queue. Pools are also supported by Fiber. They allow the user to manage a pool of worker processes. Fiber extends pools with job-backed processes so that it can manage thousands of (remote) workers per pool. Users can also create multiple pools at the same time. Figure 5: Fiber Pool. A pool with 3 workers is shown. Two of them are located on one machine and the other is located on a different machine. They collectively work on tasks that are submitted to the task queue in the master process and send results to the result queue. Managers and proxy objects enable Fiber to support shared storage, which is critical to distributed systems. Usually, this function is handled by external storage like Cassandra, Redis, etc. on a computer cluster. Fiber instead provides built-in in-memory storage for applications to use. The interface is the same as multiprocessing's Manager type. Rings are an extension to the multiprocessing API that can be helpful in distributed computing settings. A ring in Fiber stands for a group of processes who work collectively together as relative equals. Unlike Pool , Ring does not have the concept of a master process and worker processes. All the members inside the Ring share about the same responsibility. Fiber's Ring models a topology that is very common in machine learning when doing distributed SGD . Examples include torch.distributed , Horovod , etc. Generally it is very challenging to start this kind of workload on a computer cluster; Fiber provides the Ring feature to help setting up such a topology. Figure 6: Fiber Ring. A Fiber Ring with 4 nodes is depicted. Ring node 0 and ring node 3 run on the same machine but in two different containers. Ring nodes 1 and 2 both run on a separate machine. All these processes collectively run a copy of the same function and communicate with each other during the run. Applications \u00b6 Powering new applications \u00b6 Here, we show an example of how Fiber can be applied to enable large-scale distributed computation. This example is a demo of a reinforcement learning (RL) algorithm. The communication pattern for distributed RL usually involves sending different types of data between machines: actions, neural network parameters, gradients, per-step/episode observations, rewards, etc. Fiber implements pipes and pools to transmit this data. Under the hood, pools are normal Unix sockets, providing near line-speed communication for the applications using Fiber. Modern computer networking usually has bandwidth as high as hundreds of gigabits per second. Transmitting smaller amounts of data over a network is generally fast . Additionally, the inter-process communication latency does not increase much if there are many different processes sending data to one process because data transfer can happen in parallel. This fact makes Fiber's pools suitable for providing the foundation of many RL algorithms because simulators can run in each pool worker process and the results can be transmitted back in parallel. # fiber.BaseManager is a manager that runs remotely class RemoteEnvManager ( fiber . managers . AsyncManager ): pass class Env ( gym . env ): # gym env pass RemoteEnvManager . register ( 'Env' , Env ) def build_model (): # create a new policy model return model def update_model ( model , observations ): # update model with observed data return new_model def train (): model = build_model () manager = RemoteEnvManager () num_envs = 10 envs = [ manager . Env () for i in range ( num_envs )] handles = [ envs [ i ] . reset () for i in num_envs ] obs = [ handle . get () for handle in handles ] for i in range ( 1000 ): actions = model ( obs ) handles = [ env . step () for action in actions ] obs = [ handle . get () for handle in handles ] model = update_model ( model , obs ) Code Example 1: Simplified RL code implemented with Fiber Enabling existing multiprocessing applications \u00b6 Because multiprocessing is widely used in the Python world, Fiber opens up broad opportunities for such applications because now they can run in a distributed setup on a computer cluster like Kubernetes only by changing a few lines of code! Here is an example: OpenAI Baselines is a very popular library for people doing RL and it has many reference algorithms like DQN , PPO , etc. Its downside is that it only works on a single machine. If you want to train PPO on a large scale, you have to create your own MPI-based setup and manually set up the cluster to hook everything up. In contrast, with Fiber things are much easier. It can seamlessly expand RL algorithms like PPO to leverage hundreds of distributed environment workers. Fiber provides the same API as multiprocessing, which is what OpenAI Baselines uses to harvest multicore CPU processing power locally. So the change needed to make OpenAI Baselines to work with Fiber is just one line : And then you can run OpenAI Baselines on Kubernetes! We have provided a full guide for how to make the change and run Baselines on Kubernetes here . Error Handling \u00b6 Fiber implements pool-based error handling. When a new pool is created, an associated task queue, result queue, and pending table are also created. Newly-created tasks are then added to the task queue, which is shared between the master process and worker processes. Each of the workers fetches a single task from the task queue, and then runs task functions within that task. Each time a task is removed from the task queue, an entry in the pending table is added. Once the worker finishes that task, it puts its results in the result queue. The entry associated with that task is then removed from the pending table. Figure 7: Fiber Error Handling. On the left is a normal Fiber Pool with 4 workers. On the right, worker 3 fails and a new worker process (worker 5) is consequently started and ready to be added to the pool. If a pool worker process fails in the middle of processing, that failure is detected by the parent pool that serves as the process manager of all the worker processes. Then the parent pool puts the pending task from the pending table back into the task queue if the previously failed process has a pending task. Next, it starts a new worker process to replace the previously failed process and binds the newly-created worker process to the task queue and the result queue. Performance \u00b6 One of the most important applications of Fiber is to scale the computation of algorithms like RL and population-based methods like ES. In these applications, latency is critical. RL and population-based methods are typically applied in a setup that requires frequent interaction with simulators to evaluate policies and collect experiences, such as ALE , Gym , and Mujoco . The latency introduced from getting the results from the simulators critically impacts the overall training performance. In these tests, we evaluate the performance of Fiber and compare it with other frameworks. We also add Ray in our framework overhead test to provide some preliminary results, detailed results are expected to be added in the future. There are generally two ways to reduce such latency. Either we can reduce the amount of data that needs to be transferred or make the communication channel between different processes faster. For the purpose of fast communication, Fiber implements pipes and pools with Nanomsg, providing fast communication for the applications using Fiber. In addition, people can choose even higher performance with libraries like speedus . Framework overhead \u00b6 The test in this section probes how much overhead the framework adds to the workload. We compare Fiber, Python multiprocessing library, Spark, and IPyParallel. The testing procedure is to create a batch of workloads that takes a fixed amount of time in total to finish. The duration of each single task ranges from 1 second to 1 millisecond. We run five workers for each framework locally and adjust the batch size to make sure the total finish time for each framework is roughly 1 second (i.e. for 1 millisecond duration, we run 5,000 tasks). The hypothesis is that Fiber should have similar performance to multiprocessing because both of them don't reply on complex scheduling mechanisms. However, Spark and IPyParallel should be slower than Fiber because they rely on schedulers in the middle. Figure 8: Test Framework Overhead . Fiber shows almost no difference when task durations are 100ms or greater, and is much closer to multiplrocessing than the other frameworks as the task duration drops to 10 or 1ms. We use multiprocessing as a reference because it is very lightweight and does not implement any additional features beyond creating new processes and running tasks in parallel. Additionally, it exploits communication mechanisms only available locally (e.g. shared memory, Unix domain sockets, etc.), making it difficult to be surpassed by other frameworks that support distributed resource management across multiple machines and which cannot exploit similar mechanisms. It thus serves as a good reference on the performance that can be expected. Figure 9: Different frameworks compared on mean time to finish a batch of tasks with different task durations (linear scale). The optimal finishing time is 1 second. Compared to Fiber, IPyParallel and Spark fall well behind at each task duration. When the task duration is 1 millisecond, IPyParallel takes almost 24 times longer than Fiber, and Spark takes 38 times longer. This result highlights that both IPyParallel and Spark introduce considerable overhead when the task duration is short, and are not as suitable as Fiber for RL and population-based methods, where a simulator is used and the response time is a couple of milliseconds. We also show that Ray takes about 2.5x times longer than Fiber when running 1ms tasks. Distributed task test \u00b6 To probe the scalability and efficiency of Fiber, we compare it here exclusively with IPyParallel because Spark is slower than IPyParallel as shown above, and multiprocessing does not scale beyond one machine. We evaluate both frameworks on the time it takes to run 50 iterations of ES (evolution strategies) to test the scalability and efficiency of both frameworks. With the same workload, we expect Fiber to finish faster because it has much less overhead than IPyParallel as shown in the previous test. For both Fiber and IPyParallel, the population size of 2,048, so that the total computation is fixed regardless of the number of workers. The same shared noise table trick is also implemented in both. Every 8 workers share one noise table.The experimental domain in this work is a modified version of the \"Bipedal Walker Hardcore\" environment of the OpenAI Gym with modifications described in here . Figure 10: 50 Iterations of ES. Fiber scales better than IPyParallel when running ES with different number of workers. Each worker runs on a single CPU. The main result is that Fiber scales much better than IPyParallel and finishes each test significantly faster. The length of time it takes for Fiber to run gradually decreases with the increase of the number of workers from 32 to 1,024. In contrast, the time for IPyParallel to finish increases from 256 to 512 workers. IPyParallel does not finish the run with 1,024 workers due to communication errors between its processes. This failure undermines the ability for IPyParallel to run large-scale parallel computation. After 512, we saw diminishing returns for Fiber when the number of workers increased. This is because of Amdahl's law . In that case, how fast the master process can process data becomes the bottleneck. Overall, Fiber's performance exceeds IPyParallel for all numbers of workers tested. Additionally, unlike IPyParallel, Fiber also finishes the run with 1,024 workers. This result highlights Fiber's better scalability compared to IPyParallel even while it is at the same time very easy to use and set up. Conclusion \u00b6 Fiber is a new Python distributed library that is now open-sourced . It is designed to enable users to implement large scale computation easily on a computer cluster. The experiments here highlight that Fiber achieves many goals, including efficiently leveraging a large amount of heterogeneous computing hardware, dynamically scaling algorithms to improve resource usage efficiency, and reducing the engineering burden required to make complex algorithms work on computer clusters. We hope that Fiber will further enable progress in solving hard problems by making it easier to develop methods and run them at the scale necessary to truly see them shine. For more details, please checkout out our Fiber GitHub repository and Fiber paper .","title":"Introduction to Fiber"},{"location":"introduction/#the-challenge-of-large-scale-distributed-computation","text":"In an ideal world, scaling an application that runs on one machine to an application that runs on a cluster of machines should be as easy as changing a command-line argument. However, that is not an easy task in the real world. While working with many people who run large scale distributed computing jobs on a daily basis, we found that there are several reasons why it is so hard to harness distributed computing nowadays: There is a huge gap between making code work locally on laptops or desktops and running code on a production cluster. You can make MPI work locally but it's a completely different process to run it on a computer cluster. No dynamic scaling is available. If you launch a job that requires a large amount of resources, then most likely you'll need to wait until everything is allocated before you can run your job. This waiting to scale up makes it less efficient. Error handling is missing. While running, some jobs may fail. And you may be put into a very nasty situation where you have to recover part of the result or discard the whole run. High learning cost. Each system has different APIs and conventions for programming. To launch jobs with a new system, a user has to learn a set of completely new conventions before jobs can be launched. The new Fiber platform addresses each of these issues explicitly, potentially opening up seamless large-scale distributed computing to a much wider population of users.","title":"The challenge of large-scale distributed computation"},{"location":"introduction/#introducing-fiber","text":"Fiber is a Python-based distributed computing library for modern computer clusters. Instead of programming your desktop or laptop, now you can program the whole computer cluster. Originally, it was developed to power large scale parallel scientific computation projects like POET and it has been used to power similar projects within Uber. The key features of Fiber include: Easy to use. Fiber allows you to write programs that run on a computer cluster without the need to dive into the details of the computer cluster. Easy to learn. Fiber provides the same API as Python's standard multiprocessing library that people are familiar with. If you know how to use multiprocessing, you can program a computer cluster with Fiber. Fast performance. Fiber's communication backbone is built on top of Nanomsg , which is a high-performance asynchronous messaging library to allow fast and reliable communication. No need for deployment. You run Fiber application the same way as running a normal application on a computer cluster and Fiber handles the rest for you. Reliable computation. Fiber has built-in error handling when you are running a pool of workers. Users can focus on writing the actual application code instead of dealing with crashed workers. In addition, Fiber can be used together with other specialized frameworks in areas where performance is critical. Examples include distributed SGD where many existing frameworks like Horovod or torch.distributed have already provided very good solutions. Fiber can be used together with such platforms by using Fiber's Ring feature to help to set up a distributed training job on computer clusters. Figure 1: Fiber overview. The diagram shows how Fiber works on a computer cluster. It starts many different job-backed-processes and runs different Fiber components and user processes inside them. Fiber Master is the main process that manages all the other processes. Some processes like Ring Node maintain communications between each member. Fiber can (1) help users who are working on large-scale distributed computing to reduce the time to go from ideas to actually running distributed jobs on computation clusters, (2) shield users from details of configuration and resource allocation tasks, (3) enable faster debug cycles, and (4) simplify the transition from local to cluster development.","title":"Introducing Fiber"},{"location":"introduction/#architecture","text":"Fiber bridges the classical multiprocessing API with a flexible selection of backends that can run on different cluster management systems. To achieve this integration, Fiber is split into three different layers: the API layer , backend layer and cluster layer . The API layer provides basic building blocks for Fiber like processes, queues, pools and managers. They have the same semantics as in multiprocessing, but are extended to work in distributed environments. The backend layer handles tasks like creating or terminating jobs on different cluster managers. When a new backend is added, all the other Fiber components (queues, pools, etc.) do not need to be changed. Finally, the cluster layer consists of different cluster managers. Although they are not a part of Fiber itself, they help Fiber to manage resources and keep track of different jobs, thereby reducing the number of items that Fiber needs to track. This overall architecture is summarized in figure 2. Figure 2: Fiber architecture.","title":"Architecture"},{"location":"introduction/#job-backed-process","text":"Fiber introduces a new concept called job-backed processes (also called a Fiber process ). It is similar to the process in Python's multiprocessing library, but more flexible: while a process in multiprocessing only runs on a local machine, a Fiber process can run remotely on a different machine or locally on the same machine. When starting a new Fiber process, Fiber creates a new job with the proper Fiber backend on the current computer cluster. Figure 3: Job-backed processes. Each job-backed process is a containerized job running on the computer cluster. Each job-backed process will also have its own allocation of CPU, GPU and other types of resources. The code that runs inside the container is self-contained. Fiber uses containers to encapsulate the running environment of current processes, including all the required files, input data, other dependent program packages, etc., to ensure everything is self-contained. All the child processes are started with the same container image as the parent process to guarantee a consistent running environment. Because each process is a cluster job, its life cycle is the same as any job on the cluster. To make it easy for users, Fiber is designed to directly interact with computer cluster managers. Because of this, Fiber doesn't need to be set up on multiple machines or bootstrapped by any other mechanisms, unlike Spark or IPyParallel. It only needs to be installed on a single machine as a normal Python pip package.","title":"Job-Backed Process"},{"location":"introduction/#components","text":"Fiber implements most multiprocessing APIs on top of Fiber processes including pipes, queues, pools, and managers. Queues and pipes in Fiber behave the same as in multiprocessing. The difference is that queues and pipes are now shared by multiple processes running on different machines. Two processes can read from and write to the same pipe. Furthermore, queues can be shared among many processes on different machines and each process can send to or receive from the same queue at the same time. Fiber's queue is implemented with Nanomsg, a high-performance asynchronous message queue system. Figure 4: Fiber Queue. This diagram shows a Fiber queue shared across three different Fiber processes. One Fiber process is located on the same machine as the queue and the other two processes are located on another machine. One process is writing to the queue and the other two are reading from the queue. Pools are also supported by Fiber. They allow the user to manage a pool of worker processes. Fiber extends pools with job-backed processes so that it can manage thousands of (remote) workers per pool. Users can also create multiple pools at the same time. Figure 5: Fiber Pool. A pool with 3 workers is shown. Two of them are located on one machine and the other is located on a different machine. They collectively work on tasks that are submitted to the task queue in the master process and send results to the result queue. Managers and proxy objects enable Fiber to support shared storage, which is critical to distributed systems. Usually, this function is handled by external storage like Cassandra, Redis, etc. on a computer cluster. Fiber instead provides built-in in-memory storage for applications to use. The interface is the same as multiprocessing's Manager type. Rings are an extension to the multiprocessing API that can be helpful in distributed computing settings. A ring in Fiber stands for a group of processes who work collectively together as relative equals. Unlike Pool , Ring does not have the concept of a master process and worker processes. All the members inside the Ring share about the same responsibility. Fiber's Ring models a topology that is very common in machine learning when doing distributed SGD . Examples include torch.distributed , Horovod , etc. Generally it is very challenging to start this kind of workload on a computer cluster; Fiber provides the Ring feature to help setting up such a topology. Figure 6: Fiber Ring. A Fiber Ring with 4 nodes is depicted. Ring node 0 and ring node 3 run on the same machine but in two different containers. Ring nodes 1 and 2 both run on a separate machine. All these processes collectively run a copy of the same function and communicate with each other during the run.","title":"Components"},{"location":"introduction/#applications","text":"","title":"Applications"},{"location":"introduction/#powering-new-applications","text":"Here, we show an example of how Fiber can be applied to enable large-scale distributed computation. This example is a demo of a reinforcement learning (RL) algorithm. The communication pattern for distributed RL usually involves sending different types of data between machines: actions, neural network parameters, gradients, per-step/episode observations, rewards, etc. Fiber implements pipes and pools to transmit this data. Under the hood, pools are normal Unix sockets, providing near line-speed communication for the applications using Fiber. Modern computer networking usually has bandwidth as high as hundreds of gigabits per second. Transmitting smaller amounts of data over a network is generally fast . Additionally, the inter-process communication latency does not increase much if there are many different processes sending data to one process because data transfer can happen in parallel. This fact makes Fiber's pools suitable for providing the foundation of many RL algorithms because simulators can run in each pool worker process and the results can be transmitted back in parallel. # fiber.BaseManager is a manager that runs remotely class RemoteEnvManager ( fiber . managers . AsyncManager ): pass class Env ( gym . env ): # gym env pass RemoteEnvManager . register ( 'Env' , Env ) def build_model (): # create a new policy model return model def update_model ( model , observations ): # update model with observed data return new_model def train (): model = build_model () manager = RemoteEnvManager () num_envs = 10 envs = [ manager . Env () for i in range ( num_envs )] handles = [ envs [ i ] . reset () for i in num_envs ] obs = [ handle . get () for handle in handles ] for i in range ( 1000 ): actions = model ( obs ) handles = [ env . step () for action in actions ] obs = [ handle . get () for handle in handles ] model = update_model ( model , obs ) Code Example 1: Simplified RL code implemented with Fiber","title":"Powering new applications"},{"location":"introduction/#enabling-existing-multiprocessing-applications","text":"Because multiprocessing is widely used in the Python world, Fiber opens up broad opportunities for such applications because now they can run in a distributed setup on a computer cluster like Kubernetes only by changing a few lines of code! Here is an example: OpenAI Baselines is a very popular library for people doing RL and it has many reference algorithms like DQN , PPO , etc. Its downside is that it only works on a single machine. If you want to train PPO on a large scale, you have to create your own MPI-based setup and manually set up the cluster to hook everything up. In contrast, with Fiber things are much easier. It can seamlessly expand RL algorithms like PPO to leverage hundreds of distributed environment workers. Fiber provides the same API as multiprocessing, which is what OpenAI Baselines uses to harvest multicore CPU processing power locally. So the change needed to make OpenAI Baselines to work with Fiber is just one line : And then you can run OpenAI Baselines on Kubernetes! We have provided a full guide for how to make the change and run Baselines on Kubernetes here .","title":"Enabling existing multiprocessing applications"},{"location":"introduction/#error-handling","text":"Fiber implements pool-based error handling. When a new pool is created, an associated task queue, result queue, and pending table are also created. Newly-created tasks are then added to the task queue, which is shared between the master process and worker processes. Each of the workers fetches a single task from the task queue, and then runs task functions within that task. Each time a task is removed from the task queue, an entry in the pending table is added. Once the worker finishes that task, it puts its results in the result queue. The entry associated with that task is then removed from the pending table. Figure 7: Fiber Error Handling. On the left is a normal Fiber Pool with 4 workers. On the right, worker 3 fails and a new worker process (worker 5) is consequently started and ready to be added to the pool. If a pool worker process fails in the middle of processing, that failure is detected by the parent pool that serves as the process manager of all the worker processes. Then the parent pool puts the pending task from the pending table back into the task queue if the previously failed process has a pending task. Next, it starts a new worker process to replace the previously failed process and binds the newly-created worker process to the task queue and the result queue.","title":"Error Handling"},{"location":"introduction/#performance","text":"One of the most important applications of Fiber is to scale the computation of algorithms like RL and population-based methods like ES. In these applications, latency is critical. RL and population-based methods are typically applied in a setup that requires frequent interaction with simulators to evaluate policies and collect experiences, such as ALE , Gym , and Mujoco . The latency introduced from getting the results from the simulators critically impacts the overall training performance. In these tests, we evaluate the performance of Fiber and compare it with other frameworks. We also add Ray in our framework overhead test to provide some preliminary results, detailed results are expected to be added in the future. There are generally two ways to reduce such latency. Either we can reduce the amount of data that needs to be transferred or make the communication channel between different processes faster. For the purpose of fast communication, Fiber implements pipes and pools with Nanomsg, providing fast communication for the applications using Fiber. In addition, people can choose even higher performance with libraries like speedus .","title":"Performance"},{"location":"introduction/#framework-overhead","text":"The test in this section probes how much overhead the framework adds to the workload. We compare Fiber, Python multiprocessing library, Spark, and IPyParallel. The testing procedure is to create a batch of workloads that takes a fixed amount of time in total to finish. The duration of each single task ranges from 1 second to 1 millisecond. We run five workers for each framework locally and adjust the batch size to make sure the total finish time for each framework is roughly 1 second (i.e. for 1 millisecond duration, we run 5,000 tasks). The hypothesis is that Fiber should have similar performance to multiprocessing because both of them don't reply on complex scheduling mechanisms. However, Spark and IPyParallel should be slower than Fiber because they rely on schedulers in the middle. Figure 8: Test Framework Overhead . Fiber shows almost no difference when task durations are 100ms or greater, and is much closer to multiplrocessing than the other frameworks as the task duration drops to 10 or 1ms. We use multiprocessing as a reference because it is very lightweight and does not implement any additional features beyond creating new processes and running tasks in parallel. Additionally, it exploits communication mechanisms only available locally (e.g. shared memory, Unix domain sockets, etc.), making it difficult to be surpassed by other frameworks that support distributed resource management across multiple machines and which cannot exploit similar mechanisms. It thus serves as a good reference on the performance that can be expected. Figure 9: Different frameworks compared on mean time to finish a batch of tasks with different task durations (linear scale). The optimal finishing time is 1 second. Compared to Fiber, IPyParallel and Spark fall well behind at each task duration. When the task duration is 1 millisecond, IPyParallel takes almost 24 times longer than Fiber, and Spark takes 38 times longer. This result highlights that both IPyParallel and Spark introduce considerable overhead when the task duration is short, and are not as suitable as Fiber for RL and population-based methods, where a simulator is used and the response time is a couple of milliseconds. We also show that Ray takes about 2.5x times longer than Fiber when running 1ms tasks.","title":"Framework overhead"},{"location":"introduction/#distributed-task-test","text":"To probe the scalability and efficiency of Fiber, we compare it here exclusively with IPyParallel because Spark is slower than IPyParallel as shown above, and multiprocessing does not scale beyond one machine. We evaluate both frameworks on the time it takes to run 50 iterations of ES (evolution strategies) to test the scalability and efficiency of both frameworks. With the same workload, we expect Fiber to finish faster because it has much less overhead than IPyParallel as shown in the previous test. For both Fiber and IPyParallel, the population size of 2,048, so that the total computation is fixed regardless of the number of workers. The same shared noise table trick is also implemented in both. Every 8 workers share one noise table.The experimental domain in this work is a modified version of the \"Bipedal Walker Hardcore\" environment of the OpenAI Gym with modifications described in here . Figure 10: 50 Iterations of ES. Fiber scales better than IPyParallel when running ES with different number of workers. Each worker runs on a single CPU. The main result is that Fiber scales much better than IPyParallel and finishes each test significantly faster. The length of time it takes for Fiber to run gradually decreases with the increase of the number of workers from 32 to 1,024. In contrast, the time for IPyParallel to finish increases from 256 to 512 workers. IPyParallel does not finish the run with 1,024 workers due to communication errors between its processes. This failure undermines the ability for IPyParallel to run large-scale parallel computation. After 512, we saw diminishing returns for Fiber when the number of workers increased. This is because of Amdahl's law . In that case, how fast the master process can process data becomes the bottleneck. Overall, Fiber's performance exceeds IPyParallel for all numbers of workers tested. Additionally, unlike IPyParallel, Fiber also finishes the run with 1,024 workers. This result highlights Fiber's better scalability compared to IPyParallel even while it is at the same time very easy to use and set up.","title":"Distributed task test"},{"location":"introduction/#conclusion","text":"Fiber is a new Python distributed library that is now open-sourced . It is designed to enable users to implement large scale computation easily on a computer cluster. The experiments here highlight that Fiber achieves many goals, including efficiently leveraging a large amount of heterogeneous computing hardware, dynamically scaling algorithms to improve resource usage efficiency, and reducing the engineering burden required to make complex algorithms work on computer clusters. We hope that Fiber will further enable progress in solving hard problems by making it easier to develop methods and run them at the scale necessary to truly see them shine. For more details, please checkout out our Fiber GitHub repository and Fiber paper .","title":"Conclusion"},{"location":"managers/","text":"fiber.managers Managers and proxy objects enable multiprocessing to support shared storage, which is critical to distributed systems. Usually, this function is handled by external storage systems like Cassandra, Redis, etc. Fiber instead provides built-in in-memory storage for applications to use (based on multiprocessing.managers). The interface is the same as multiprocessing's Manager type. BaseManager BaseManager ( self , address = None , authkey = None , serializer = 'pickle' , ctx = None ) A Base class for people to create customized managers . Equivalent to multiprocessing.managers.BaseManager but can work on a computer cluster. The API of this class is the same as multiprocessing.managers.BaseManager SyncManager SyncManager ( self , address = None , authkey = None , serializer = 'pickle' , ctx = None ) Subclass of BaseManager which supports a number of shared object types. The types that are supported include: Queue JoinableQueue list dict Value Array Namespace AsyncResult SyncManager is exposed as fiber.Manager . You can also use fiber.managers.SyncManager directly. The difference is that fiber.Manager will start the newly created manager and when using SyncManager , the user need to call the start() method of the manager explicitly. fiber.Manager example: manager = fiber.Manager() d = manager.dict() d[\"name\"] = \"managed_dict\" print(d) fiber.managers.SyncManager example: manager = fiber.managers.SyncManager() manager.start() l = manager.list() l.append(\"x\") print(l) AsyncManager AsyncManager ( self , address = None , authkey = None , serializer = 'pickle' , ctx = None ) Asynchronous version of BaseManager. Currently, this type doesn't register any shared object types like SyncManager, it is more meant to be used as a base class for customized managers . The API of the class is the same as SyncManager and multiprocessing.managers.BaseManager Example usage: from fiber.managers import AsyncManager class Calculator (): def add ( self , a , b ): return a + b class MyManager ( AsyncManager ): pass MyManager . register ( \"Calculator\" , Calculator ) manager = MyManager () manager . start () calc = manager . Calculator () res = calc . add ( 10 , 32 ) print ( type ( res )) # fiber.managers.AsyncProxyResult print ( res . get ()) # 42","title":"Managers"},{"location":"meta/","text":"meta meta ( ** kwargs ) fiber.meta API allows you to decorate your function and provide some hints to Fiber. Currently this is mainly used for specify the resource usage of user functions. Currently, support keys are: key Type Default Notes cpu int None The number of CPU cores that this function needs memory int None The size of memory space in MB this function needs gpu int None The number of GPUs that this function needs. For how to setup Fiber with GPUs, check out here Example usage: @fiber . meta ( cpu = 4 , memory = 1000 , gpu = 1 ) def func (): do_something ()","title":"Metadata"},{"location":"misc/","text":"init init ( ** kwargs ) Initialize Fiber. This function is called when you want to re-initialize Fiber with new config values and also re-init loggers. Arguments : kwargs : If kwargs is not None, init Fiber system with corresponding key/value pairs in kwargs as config keys and values.","title":"Misc"},{"location":"platforms/","text":"Supported platforms \u00b6 Currently the following platforms are supported: Operating system: Linux, MacOS (local backend only) Python: 3.6+ Supported cluster management systems: Kubernetes (Tested with Google Kubernetes Engine on Google cloud) We are interested in supporting other cluster management systems like Slurm , if you want to contribute to it please let us know. Different fiber backends \u00b6 Fiber provides 3 built-in backends currently: local \u00b6 This backend is the default backend used by Fiber. It starts new processes locally on your computer. The child process shares the same running environment as the parent process. When using this backend, Fiber works the same as multiprocessing . The local backend works on Linux and MacOS. docker \u00b6 This backend starts new processes with docker containers. This backend still works locally but compared to local backend, docker backend runs child processes in a completed isolated docker environment. You need to build a docker image for your project before running processes with this backend. Examples of using this backend can be found here . This backend is usually used as a way to test your containerized application before you run everything on a large scale on a computer cluster. The docker backend currently only works on Linux and doesn't work on MacOS, we will add support for it soon. kubernetes \u00b6 This backend is the backend to use when you want to run your application on Kubernetes cluster. When starting new processes, it creates a new Kubernetes pod for it. Each process also runs in an isolated environment. You also need to build a docker image and push it to the appropriate container image registry for your Kubernetes platform. Example usage of this backend can be found here . See how to configure Fiber to use each backend here Automatic backend selection \u00b6 Fiber also supports automatically detect which backend to use when no backend is explicitly set. When fiber runs on a non-cluster environment, it uses local backend by default. When it runs inside a Kubernetes pod, it will select kubernetes as the backend to use.","title":"Platforms and Backends"},{"location":"platforms/#supported-platforms","text":"Currently the following platforms are supported: Operating system: Linux, MacOS (local backend only) Python: 3.6+ Supported cluster management systems: Kubernetes (Tested with Google Kubernetes Engine on Google cloud) We are interested in supporting other cluster management systems like Slurm , if you want to contribute to it please let us know.","title":"Supported platforms"},{"location":"platforms/#different-fiber-backends","text":"Fiber provides 3 built-in backends currently:","title":"Different fiber backends"},{"location":"platforms/#local","text":"This backend is the default backend used by Fiber. It starts new processes locally on your computer. The child process shares the same running environment as the parent process. When using this backend, Fiber works the same as multiprocessing . The local backend works on Linux and MacOS.","title":"local"},{"location":"platforms/#docker","text":"This backend starts new processes with docker containers. This backend still works locally but compared to local backend, docker backend runs child processes in a completed isolated docker environment. You need to build a docker image for your project before running processes with this backend. Examples of using this backend can be found here . This backend is usually used as a way to test your containerized application before you run everything on a large scale on a computer cluster. The docker backend currently only works on Linux and doesn't work on MacOS, we will add support for it soon.","title":"docker"},{"location":"platforms/#kubernetes","text":"This backend is the backend to use when you want to run your application on Kubernetes cluster. When starting new processes, it creates a new Kubernetes pod for it. Each process also runs in an isolated environment. You also need to build a docker image and push it to the appropriate container image registry for your Kubernetes platform. Example usage of this backend can be found here . See how to configure Fiber to use each backend here","title":"kubernetes"},{"location":"platforms/#automatic-backend-selection","text":"Fiber also supports automatically detect which backend to use when no backend is explicitly set. When fiber runs on a non-cluster environment, it uses local backend by default. When it runs inside a Kubernetes pod, it will select kubernetes as the backend to use.","title":"Automatic backend selection"},{"location":"pool/","text":"fiber.pool Pools are supported by Fiber. They allow the user to manage a pool of worker processes. Fiber extend pools with job-backed processes so that it can manage thousands of (remote) workers per pool. Users can also create multiple pools at the same time. Fiber implements 2 different version of Pool : ZPool and ResilientZPool . Both has the same API as multiprocessing.Pool . ZPool is pool based on \"r\"/\"w\" socket pairs. ResilientZPool is ZPool + error handling . Failed tasks will be resubmitted to the Pool and worked on by other pool workers. By default, ResilientZPool is exposed as fiber.Pool . Example: pool = fiber . Pool ( processes = 4 ) pool . map ( math . sqrt , range ( 10 )) ApplyResult ApplyResult ( self , seq , inventory ) An object that is returned by asynchronous methods of Pool . It represents an handle that can be used to get the actual result. get ApplyResult . get () Get the actual result represented by this object Returns : Actual result. This method will block if the actual result is not ready. MapResult MapResult ( self , seq , inventory ) ZPool ZPool ( self , processes = None , initializer = None , initargs = (), maxtasksperchild = None , cluster = None , master_sock_type = 'w' ) A Pool implementation based on Fiber sockets. ZPool directly uses Fiber sockets instead of SimpleQueue for tasks and results handling. This makes it faster. apply_async ZPool . apply_async ( func , args = (), kwds = {}, callback = None , error_callback = None ) Run function func with arguments args and keyword arguments kwds on a remote Pool worker. This is an asynchronous version of apply . Arguments : func : target function to run. args : positional arguments that needs to be passed to func . kwds : keyword arguments that needs to be passed to func . callback : Currently not supported. A callback function that will be called when the result is ready. error_callback : Currently not supported. A callback function that will be called when an error occurred. Returns : An ApplyResult object which has a method .get() to get the actual results. map_async ZPool . map_async ( func , iterable , chunksize = None , callback = None , error_callback = None ) For each element e in iterable , run func(e) . The workload is distributed between all the Pool workers. This is an asynchronous version of map . Arguments : func : target function to run. iterable : an iterable object to be mapped. chunksize : if set, elements in iterable will be put in to chunks whose size is decided by chunksize . These chunks will be sent to Pool workers instead of each elements in iterable . If not set, the chunksize is decided automatically. callback : Currently not supported. A callback function that will be called when the result is ready. error_callback : Currently not supported. A callback function that will be called when an error occurred. Returns : An MapResult object which has a method .get() to get the actual results. apply ZPool . apply ( func , args = (), kwds = {}) Run function func with arguments args and keyword arguments kwds on a remote Pool worker. Arguments : func : target function to run. args : positional arguments that needs to be passed to func . kwds : keyword arguments that needs to be passed to func . Returns : the return value of func(*args, **kwargs) . map ZPool . map ( func , iterable , chunksize = None ) For each element e in iterable , run func(e) . The workload is distributed between all the Pool workers. Arguments : func : target function to run. iterable : an iterable object to be mapped. chunksize : if set, elements in iterable will be put in to chunks whose size is decided by chunksize . These chunks will be sent to Pool workers instead of each elements in iterable . If not set, the chunksize is decided automatically. Returns : A list of results equivalent to calling [func(x) for x in iterable] . imap ZPool . imap ( func , iterable , chunksize = 1 ) For each element e in iterable , run func(e) . The workload is distributed between all the Pool workers. This function returns an iterator which user and iterate over to get results. Arguments : func : target function to run. iterable : an iterable object to be mapped. chunksize : if set, elements in iterable will be put in to chunks whose size is decided by chunksize . These chunks will be sent to Pool workers instead of each elements in iterable . If not set, the chunksize is decided automatically. Returns : an iterator which user can use to get results. imap_unordered ZPool . imap_unordered ( func , iterable , chunksize = 1 ) For each element e in iterable , run func(e) . The workload is distributed between all the Pool workers. This function returns an unordered iterator which user and iterate over to get results. This means that the order of the results may not match the order of the iterable . Arguments : func : target function to run. iterable : an iterable object to be mapped. chunksize : if set, elements in iterable will be put in to chunks whose size is decided by chunksize . These chunks will be sent to Pool workers instead of each elements in iterable . If not set, the chunksize is decided automatically. Returns : an unordered iterator which user can use to get results. starmap_async ZPool . starmap_async ( func , iterable , chunksize = None , callback = None , error_callback = None ) For each element args in iterable , run func(*args) . The workload is distributed between all the Pool workers. This is an asynchronous version of starmap . For example, starmap_async(func, [(1, 2, 3), (4, 5, 6)]) will result in calling func(1, 2, 3) and func(4, 5, 6) on a remote host. Arguments : func : target function to run. iterable : an iterable object to be mapped. chunksize : if set, elements in iterable will be put in to chunks whose size is decided by chunksize . These chunks will be sent to Pool workers instead of each elements in iterable . If not set, the chunksize is decided automatically. callback : Currently not supported. A callback function that will be called when the result is ready. error_callback : Currently not supported. A callback function that will be called when an error occurred. Returns : An MapResult object which has a method .get() to get the actual results. starmap ZPool . starmap ( func , iterable , chunksize = None ) For each element args in iterable , run func(*args) . The workload is distributed between all the Pool workers. For example, starmap_async(func, [(1, 2, 3), (4, 5, 6)]) will result in calling func(1, 2, 3) and func(4, 5, 6) on a remote host. Arguments : func : target function to run. iterable : an iterable object to be mapped. chunksize : if set, elements in iterable will be put in to chunks whose size is decided by chunksize . These chunks will be sent to Pool workers instead of each elements in iterable . If not set, the chunksize is decided automatically. callback : Currently not supported. A callback function that will be called when the result is ready. error_callback : Currently not supported. A callback function that will be called when an error occurred. Returns : A list of results equivalent to calling [func(*arg) for arg in iterable] close ZPool . close () Close this Pool. This means the current pool will be put in to a closing state and it will not accept new tasks. Existing workers will continue to work on tasks that have been dispatched to them and exit when all the tasks are done. terminate ZPool . terminate () Terminate this pool. This means that this pool will be terminated and all its pool workers will also be terminated. Task that have been dispatched will be discarded. join ZPool . join () Wait for all the pool workers of this pool to exit. This should be used after terminate() or close() are called on this pool. ResilientZPool ResilientZPool ( self , processes = None , initializer = None , initargs = (), maxtasksperchild = None , cluster = None ) ZPool with error handling. The differences are: Master socket is a ROUTER socket instead of DEALER socket. Add pending table. When an died worker is detected, it's jobs are resubmitted to work Q in addition to restarting that worker. The API of ResilientZPool is the same as ZPool . One difference is that if processes argument is not set, its default value is 1.","title":"Pool"},{"location":"process/","text":"fiber.process Fiber introduces a new concept called job-backed processes . It is similar to the process in Python's multiprocessing library, but more flexible: while a process in multiprocessing only runs on a local machine, a Fiber process can run remotely on a different machine or locally on the same machine. When starting a new Fiber process, Fiber creates a new job with the proper Fiber backend on the current computer cluster. Fiber uses containers to encapsulate the running environment of current processes, including all the required files, input data, and other dependent program packages, etc., to ensure everything is self-contained. All the child processes are started with the same container image as the parent process to guarantee a consistent running environment. Because each process is a cluster job, its life cycle is the same as any job on the cluster. Process Process ( self , group = None , target = None , name = None , args = (), kwargs = {}, * , daemon ) Create and manage Fiber processes. The API is compatible with Python's multiprocessing.Process, check here for multiprocessing's documents. Example usage: p = Process ( target = f , args = ( 'Fiber' ,)) p . start () pid \u00b6 Process.pid Return the current process PID. If the process hasn't been fully started, the value will be None . This PID is assigned by Fiber and is different from the operating system process PID. The value of pid is derived from the job ID of the underlying job that runs this Fiber process. name \u00b6 Process.name Return the name of this Fiber process. This value need to be set before the start of the Process. Example: p = fiber . Process ( target = print , args = ( \"demo\" )) p . name = \"DemoProcess\" daemon \u00b6 Process.daemon In multiprocessing, if a process has daemon set, it will be terminated when it's parent process exits. In Fiber, current this value has no effect, all processes will be cleaned up when their parent exits. authkey \u00b6 Process.authkey Authkey is used to authenticate between parent and child processes. It is a byte string. exitcode \u00b6 Process.exitcode The exit code of current process. If the process has not exited, the exitcode is None . If the current process has exited, the value of this is an integer. sentinel \u00b6 Process.sentinel Returns a file descriptor that becomes \"ready\" when the process exits. You can call select and other eligible functions that works on fds on this file descriptor. run Process . run () Run the target function of current process (in current process) Returns : return value of the target function start Process . start () Start this process. Under the hood, Fiber calls the API on the computer cluster to start a new job and run the target function in the new job. terminate Process . terminate () Terminate current process. When running locally, Fiber sends an SIGTERM signal to the child process. When running on a computer cluster, Fiber calls the corresponding API on that platform to terminate the job that runs Fiber process. join Process . join ( timeout = None ) Wait for this process to terminate. Arguments : timeout : The maximum duration of time in seconds that this call should wait before return. if timeout is None (default value), this method will wait forever until the process terminates. If timeout is 0 , it will check if the process has exited and return immediately. Returns : The exit code of this process is_alive Process . is_alive () Check if current process is still alive Returns : True if current process is still alive. Returns False if it's not alive. active_children active_children () Get a list of children processes of the current process. Returns : A list of children processes. Example: p = fiber . Process ( target = time . sleep , args = ( 10 ,)) p . start () print ( fiber . active_children ()) current_process current_process () Return a Process object representing the current process. Example: print ( fiber . current_process ())","title":"Process"},{"location":"process/#pid","text":"Process.pid Return the current process PID. If the process hasn't been fully started, the value will be None . This PID is assigned by Fiber and is different from the operating system process PID. The value of pid is derived from the job ID of the underlying job that runs this Fiber process.","title":"pid"},{"location":"process/#name","text":"Process.name Return the name of this Fiber process. This value need to be set before the start of the Process. Example: p = fiber . Process ( target = print , args = ( \"demo\" )) p . name = \"DemoProcess\"","title":"name"},{"location":"process/#daemon","text":"Process.daemon In multiprocessing, if a process has daemon set, it will be terminated when it's parent process exits. In Fiber, current this value has no effect, all processes will be cleaned up when their parent exits.","title":"daemon"},{"location":"process/#authkey","text":"Process.authkey Authkey is used to authenticate between parent and child processes. It is a byte string.","title":"authkey"},{"location":"process/#exitcode","text":"Process.exitcode The exit code of current process. If the process has not exited, the exitcode is None . If the current process has exited, the value of this is an integer.","title":"exitcode"},{"location":"process/#sentinel","text":"Process.sentinel Returns a file descriptor that becomes \"ready\" when the process exits. You can call select and other eligible functions that works on fds on this file descriptor.","title":"sentinel"},{"location":"queues/","text":"fiber.queues Queues and pipes in Fiber behave the same as in multiprocessing. The difference is that queues and pipes are now shared by multiple processes running on different machines. Two processes can read from and write to the same pipe. Furthermore, queues can be shared between many processes on different machines and each process can send to or receive from the same queue at the same time. Fiber's queue is implemented with Nanomsg , a high-performance asynchronous message queue system. We implemented two version of SimpleQueue : SimpleQueuePush and SimpleQueuePull . To decide which queue to use, you can set fiber.config.use_push_queue to True to use SimpleQueuePush or False to use SimpleQueuePull . The difference between SimpleQueuePush and SimpleQueuePull is whether data is push from master process to sub-processes or data is requested by sub-processes. When SimpleQueuePull is used, sub-processes will send an message to the master process to notify master process that it is ready. And the master process will send the data to sub-processes. In this way, data in the queue are load-balanced between different sub-processes due to the workload of them. When SimpleQueuePush is used, all the data are push to the sub-processes as soon as possible and the data are load-balanced in a round-robin manner. This way, there is less overhead for sending the data to the client, but there may be uneven load between each of the sub-processes. Example: q = fiber . SimpleQueue () q . put ( 42 ) # By default, `SimpleQueue` is `SimpleQueuePull`, now we switch to # `SimpleQueuePush` fiber . config . use_push_queue = True q = fiber . SimpleQueue () SimpleQueuePush SimpleQueuePush ( self ) A queue build on top of Fiber socket. It uses \"w\" - (\"r\" - \"w\") - \"r\" socket combination. Messages are pushed from one end of the queue to the other end without explicitly pulling. get SimpleQueuePush . get () Get an element from this Queue. Returns : An element from this queue. If there is no element in the queue, this method will block. put SimpleQueuePush . put ( obj ) Put an element into the Queue. Arguments : obj : Any picklable Python object. Pipe Pipe ( duplex = True ) Return a pair of connected ZConnection objects. Arguments : duplex : if duplex, then both read and write are allowed on each of the returned connection object. Otherwise, the first returned connection will be read-only and the second connection will be write-only. By default, duplex is enabled. ZConnection ZConnection ( self , handle , readable = True , writable = True ) A Connection class implemented with Fiber socket. It takes a (sock_type, dest_addr) tuple as input, creates a new connection which can send and recv Python objects. This ZConnection class can be serialized by pickle and then send to another remote process. Fiber socket will be reconnected after unpickle. Arguments : handle : a (sock_type, dest_addr) tuple. sock_type should be a Fiber socket type. dest_addr should be in format like \"tcp://127.0.0.1:9000\". note: ZConnection's fileno method returns a Fiber socket.","title":"Queues"},{"location":"why/","text":"Current problems \u00b6 It is well known that doing distributed computing is hard. After talking to many people who run large scale distributed computing jobs on a daily basis, we found that there are a couple of reasons why it is so hard to do distributed computing nowadays: There is a huge gap between making code work locally on laptops or desktops and running code on a production cluster. You can make MPI work locally but it's a completely different story for running it on a computer cluster. No dynamic scaling is available. If you launch a job that requires a large amount of resources, then most likely you'll need to wait until everything is allocated before you can run your job. This makes it less efficient. Error handling is missing. While running, some jobs may fail. And you may be put into a very nasty situation where you have to recover part of the result or discard the whole run. High learning cost. Each system has different APIs and ways of programming. In order to launch jobs with a new system, a user has to learn a set of completely different knowledge before jobs can be launched. Limitation in the programming model. Sometimes it's hard to make framework A work on framework B because the programming model of framework B is not compatible with framework A. How can Fiber help \u00b6 After we understood why it's hard, the path for designing a better framework for distributed computing is clear. We need to make it easy to use and it needs to have very low friction between local development and remote run. And Fiber help to solve these problems by: Providing the same API when running locally on a desktop or on a computer cluster, reducing the frictions between local development and running on computer clusters Providing built-in ways of dynamically creating new jobs (processes), allowing computation to dynamically scale while running Handling job-level errors (job failed/rescheduling) while processes are running Reusing the same APIs as Python\u2019s multiprocessing library. If users are familiar with how to do parallel computing with Python\u2019s multiprocessing, they can use Fiber. This greatly reduced learning cost. And also, because multiprocessing is Python's standard library, by reusing its API, it increased the probability that Fiber is compatible with other frameworks. Fiber doesn't need to be deployed. It's designed to be a library instead of a service. User's don't need to re-deploy their running environment if they added some new dependency to their code. In addition to providing an easy to use user interface, Fiber is also built with performance in mind. We build Fiber's communication backbone with Nanomsg , a high-performance asynchronous messaging library. In addition, we also make sure that Fiber can be used together with other specialized frameworks in areas where performance is critical. Examples of this include distributed SGD where many existing frameworks like Horovod , torch.distributed have already provided very good solutions. Fiber can be used together with them using Fiber's Ring feature to help to set up a distributed training job on computer clusters.","title":"Why Use Fiber"},{"location":"why/#current-problems","text":"It is well known that doing distributed computing is hard. After talking to many people who run large scale distributed computing jobs on a daily basis, we found that there are a couple of reasons why it is so hard to do distributed computing nowadays: There is a huge gap between making code work locally on laptops or desktops and running code on a production cluster. You can make MPI work locally but it's a completely different story for running it on a computer cluster. No dynamic scaling is available. If you launch a job that requires a large amount of resources, then most likely you'll need to wait until everything is allocated before you can run your job. This makes it less efficient. Error handling is missing. While running, some jobs may fail. And you may be put into a very nasty situation where you have to recover part of the result or discard the whole run. High learning cost. Each system has different APIs and ways of programming. In order to launch jobs with a new system, a user has to learn a set of completely different knowledge before jobs can be launched. Limitation in the programming model. Sometimes it's hard to make framework A work on framework B because the programming model of framework B is not compatible with framework A.","title":"Current problems"},{"location":"why/#how-can-fiber-help","text":"After we understood why it's hard, the path for designing a better framework for distributed computing is clear. We need to make it easy to use and it needs to have very low friction between local development and remote run. And Fiber help to solve these problems by: Providing the same API when running locally on a desktop or on a computer cluster, reducing the frictions between local development and running on computer clusters Providing built-in ways of dynamically creating new jobs (processes), allowing computation to dynamically scale while running Handling job-level errors (job failed/rescheduling) while processes are running Reusing the same APIs as Python\u2019s multiprocessing library. If users are familiar with how to do parallel computing with Python\u2019s multiprocessing, they can use Fiber. This greatly reduced learning cost. And also, because multiprocessing is Python's standard library, by reusing its API, it increased the probability that Fiber is compatible with other frameworks. Fiber doesn't need to be deployed. It's designed to be a library instead of a service. User's don't need to re-deploy their running environment if they added some new dependency to their code. In addition to providing an easy to use user interface, Fiber is also built with performance in mind. We build Fiber's communication backbone with Nanomsg , a high-performance asynchronous messaging library. In addition, we also make sure that Fiber can be used together with other specialized frameworks in areas where performance is critical. Examples of this include distributed SGD where many existing frameworks like Horovod , torch.distributed have already provided very good solutions. Fiber can be used together with them using Fiber's Ring feature to help to set up a distributed training job on computer clusters.","title":"How can Fiber help"},{"location":"experimental/ring/","text":"fiber.experimental.ring Ring allows you to create a Ring like topology easily on a computer cluster. RingNode RingNode ( self , rank ) A RingNode represents a node in the Ring . Arguments : rank : The id assigned to this node. Each node will be assigned a unique id called rank . Rank 0 is the control node of the Ring . Ring Ring ( self , processes , func , initializer , initargs = None ) Create a ring of nodes (processes). Each node runs a copy of the same function. Arguments : processes : Number of ring nodes in this Ring. func : the target function that each ring node will run. initializer : the initialization function that each ring node runs. This is used to setup the custom ring structure like PyTorch's torch.distributed, Horovod, etc. initargs : positional arguments that are passed to initializer. Currently this is not used. run Ring . run () Start this Ring. This will start the ring 0 process on the same machine and start all the other ring nodes with Fiber processes.","title":"Ring"}]}